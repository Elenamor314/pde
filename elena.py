# -*- coding: utf-8 -*-
""".ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YVEux5J7wvWqAtov6QLzG_g3r9qulHUw
"""

!pip install roboflow

!pip install "ultralytics<=8.3.40"

!git clone https://github.com/ultralytics/ultralytics

!cd ultralytics && ls
import os
os.chdir("ultralytics")
print(os.getcwd())

!pip install -e .

import requests
import zipfile
from pathlib import Path
from google.colab import drive
import shutil
import os

def Mount_drive(data_path) -> str:
  data_path_1 = data_path
  if os.path.exists(data_path):
    print(f"Drive is already mounted")
  else :
    drive.mount(data_path_1)


Mount_drive("/content/dataset_1")


drive_file = Path("/content/dataset_1/MyDrive/YOLO_People_Detection")
dataset_path_2 = drive_file / "PeopleDetection.v8i.yolov8.zip"


dataset_file = Path("collection/")
dataset = dataset_file / "data"
dataset.mkdir(parents = True , exist_ok = True)

# if not any(dataset.iterdir()):
#   #shutil.rmtree(dataset)
#   #dataset.mkdir()
#   with zipfile.ZipFile(dataset_path_2 , "r") as zip_ref :
#     print("Unzipping data ...")
#     zip_ref.extractall(dataset)
# else :
#   dataset.mkdir(parents = True , exist_ok = True)
#   #mount the data into spcified colab directory :
# #!unzip /content/drive/MyDrive/yolov8s_project_wit./h_Openlmanges_dataset/Vehicles-OpenImages.v1-416x416.yolov8.zip -d /image_path/dataset
#   #!unzip data_set_path -d /dataset
#   with zipfile.ZipFile(dataset_path_2 , "r") as zip_ref :
#     print("Unzipping data ...")
#     zip_ref.extractall(dataset)

def extract_ignoring_errors(zip_file, destination):
    """Extracts a zip file, ignoring errors due to corrupted files."""
    try:
        with zipfile.ZipFile(zip_file, "r") as zip_ref:
            for member in zip_ref.namelist():
                try:
                    zip_ref.extract(member, destination)
                except zipfile.BadZipFile as e:
                    print(f"Ignoring error for file '{member}': {e}")
    except zipfile.BadZipFile as e:
        print(f"Error extracting zip file: {e}")



if not any(dataset.iterdir()):

    extract_ignoring_errors(dataset_path_2, dataset) # Call the new function
else:

    extract_ignoring_errors(dataset_path_2, dataset) # Call the new function

import os
def walk_through_dir(dir_path):
  """
  Walks through dir_path returning its contents.
  Args:
    dir_path (str or pathlib.Path): target directory

  Returns:
    A print out of:
      number of subdiretories in dir_path
      number of images (files) in each subdirectory
      name of each subdirectory
  """
  for dirpath, dirnames, filenames in os.walk(dir_path):
    print(f"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.")


walk_through_dir(dataset)

import ultralytics

from ultralytics import YOLO

!pip install clearml

api {
  # Elena Elena's workspace
  web_server: https://app.clear.ml/
  api_server: https://api.clear.ml
  files_server: https://files.clear.ml
  credentials {
    "access_key" = "X7MNG6WNMEMSVT8FROC77UQGD6VIPF"
    "secret_key" = "wrkS2WNMr0BGfEo7NJbPs4cDkjt1tvZ9GocBs92awB7MdYyPPKaQezlfLvtZJlApS5M"
  }
}

!clearml-init

from clearml import Task
from ultralytics import YOLO

# init clearml
task = Task.init(project_name="my_project", task_name="my_yolov8_task_resume_second_training_Session_35_more_epochs")

#model variant
model_variant = "yolov8s"
task.set_parameter("model_variant", model_variant)

model = YOLO("/content/epoch52.pt")  #Adjust
args = dict(
    data="/content/ultralytics/collection/data/data.yaml",
    epochs=90,
    batch=8,
    imgsz=640,
    save_period=2,
    seed=314466159,
    plots=True,
    resume=True
)

task.connect(args)  # lpg
results = model.train(**args)

from IPython.display import display
display(None)

!ls /content/last (4).pt

!ls /content/last4.pt

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d dimensi0n/imagenet-256 -p /content/

function ClickConnect(){
  console.log("Clicking");
  document.querySelector("colab-connect-button").shadowRoot.querySelector("#connect").click()
}
setInterval(ClickConnect, 60000)

"""# Mount Drive and Cat data"""

from google.colab import drive
drive.mount("/content/drive")

!apt-get install p7zip-full

import os

from pathlib import Path
dataset_file = Path("collection/")
dataset1 = dataset_file / "dataset_1"
dataset1.mkdir(parents = True , exist_ok = True)

!ls /content/drive/MyDrive/ImageNet500k

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/ImageNet500k/

!7z x archive_2.7z.001 -o/content/collection/dataset_1/







!cat /content/drive/MyDrive/ImageNet500k/archive_2.7z.* > /content/drive/MyDrive/ImageNet500k/imagenet-256_full.zip

# Create a new directory to combine datasets
!mkdir -p /content/drive/MyDrive/MergedDataset/

# Copy contents of Dataset1, Dataset2, and Dataset3 into MergedDataset
!cp -r /content/drive/MyDrive/Dataset1/* /content/drive/MyDrive/MergedDataset/
!cp -r /content/drive/MyDrive/Dataset2/* /content/drive/MyDrive/MergedDataset/
!cp -r /content/drive/MyDrive/Dataset3/* /content/drive/MyDrive/MergedDataset/

# duplicate not should be here
# Copy Dataset1 and rename the files to avoid duplicates
!find /content/drive/MyDrive/Dataset1/ -type f -exec sh -c 'cp {} /content/drive/MyDrive/MergedDataset/$(basename "{}")' \;

# Copy Dataset2 and rename the files to avoid duplicates (e.g., add _dataset2 to the name)
!find /content/drive/MyDrive/Dataset2/ -type f -exec sh -c 'cp {} /content/drive/MyDrive/MergedDataset/$(basename "{}" .jpg)_dataset2.jpg' \;

# Copy Dataset3 and rename the files to avoid duplicates (e.g., add _dataset3 to the name)
!find /content/drive/MyDrive/Dataset3/ -type f -exec sh -c 'cp {} /content/drive/MyDrive/MergedDataset/$(basename "{}" .jpg)_dataset3.jpg' \;

from pathlib import Path

dataset_Image = Path("/content/ImageNet500/imagenet")
dataset_Image.mkdir(parents = True , exist_ok = True )



def extract_ignoring_errors(zip_file, destination):
    """Extracts a zip file, ignoring errors due to corrupted files."""
    try:
        with zipfile.ZipFile(zip_file, "r") as zip_ref:
            for member in zip_ref.namelist():
                try:
                    zip_ref.extract(member, destination)
                except zipfile.BadZipFile as e:
                    print(f"Ignoring error for file '{member}': {e}")
    except zipfile.BadZipFile as e:
        print(f"Error extracting zip file: {e}")

data = Path("/content/collection/dataset_1/archive.zip")

import requests
import zipfile
from pathlib import Path
from google.colab import drive
import shutil
import os



if not any(dataset_Image.iterdir()):

    extract_ignoring_errors(data, dataset_Image) # Call the new function
else:

    extract_ignoring_errors(data, dataset_Image) # Call the new function

# Copy subdirectories from Dataset1, Dataset2, and Dataset3 into MergedDataset
!cp -r /content/drive/MyDrive/dataset1/train /content/drive/MyDrive/MergedDataset/train
!cp -r /content/drive/MyDrive/dataset2/train /content/drive/MyDrive/MergedDataset/train
!cp -r /content/drive/MyDrive/dataset3/train /content/drive/MyDrive/MergedDataset/train

Elenamorshedloo314$

import os
from collections import defaultdict

def count_images_per_class(root_dir):
    class_counts = defaultdict(int)

    for class_name in os.listdir(root_dir):
        class_path = os.path.join(root_dir, class_name)
        if os.path.isdir(class_path):
            image_files = [
                f for f in os.listdir(class_path)
                if f.lower().endswith((".jpg", ".jpeg", ".png"))
            ]
            class_counts[class_name] = len(image_files)

    return dict(sorted(class_counts.items(), key=lambda item: item[1], reverse=True))


# Example usage:
root_dir = dataset_Image
counts = count_images_per_class(root_dir)

# Print results
for cls, count in counts.items():
    print(f"{cls}: {count} images")
    print(f"{len(counts)}")

# Optionally: save to a file
# with open("class_distribution.txt", "w") as f:
#     for cls, count in counts.items():
#         f.write(f"{cls}: {count} images\n")

import os
def walk_through_dir(dir_path):

  for dirpath , dirnames , filenames in os.walk(dir_path) :
    print(f"there are {len(dirnames)} directories , and {len(filenames)} images in {dirpath}")

walk_through_dir(dataset_Image)

os.getcwd()

import os
import random
from pathlib import Path
from collections import defaultdict

def create_stratified_split(root_dir, train_per_class=30, test_per_class=5, seed=42, save_to="selected_images.txt"):
    random.seed(seed)
    image_dict = defaultdict(list)

    root_path = Path(root_dir)

    # Group images by class
    for class_dir in root_path.iterdir():
        if class_dir.is_dir():
            class_name = class_dir.name
            images = list(class_dir.glob("*.jpg"))
            if len(images) >= (train_per_class + test_per_class):
                selected = random.sample(images, train_per_class + test_per_class)
                train_imgs = selected[:train_per_class]
                test_imgs = selected[train_per_class:]
                for img in train_imgs:
                    image_dict["train"].append(str(img))
                for img in test_imgs:
                    image_dict["test"].append(str(img))

    # Save to file
    with open(save_to, "w") as f:
        for img in image_dict["train"]:
            f.write(f"train {img}\n")
        for img in image_dict["test"]:
            f.write(f"test {img}\n")

    print(f"Saved stratified split with {len(image_dict['train'])} train and {len(image_dict['test'])} test images.")

from collections import defaultdict
import random

root_path = Path("/content/ImageNet500/imagenet")

train_per_class = 30
test_per_class = 5

image_dict = defaultdict(list)

for class_dir in root_path.iterdir():
        if class_dir.is_dir():
            class_name = class_dir.name
            images = list(class_dir.glob("*.jpg"))
            if len(images) >= (train_per_class + test_per_class):
                selected = random.sample(images, train_per_class + test_per_class)
                train_imgs = selected[:train_per_class]
                test_imgs = selected[train_per_class:]
                for img in train_imgs:
                    image_dict["train"].append(str(img))
                for img in test_imgs:
                    image_dict["test"].append(str(img))

a = image_dict["test"]
len(a)

os.makedirs(selected_images.txt , exist_ok = True)

create_stratified_split(
    root_dir="/content/ImageNet500/imagenet",
    train_per_class=30,
    test_per_class=5,
    save_to="selected_images.txt"
)



"""# a better dataset"""

import torch
from torch.utils.data import Dataset
from torchvision import transforms
from PIL import Image
import os
import random
from pathlib import Path

class StratifiedFeatureDataset(Dataset):
    def __init__(self, selected_file, mode, cache_dir, current_epoch, total_epochs = 100 ,  transform=None, noise_factor=0.25, seed=42):
        assert mode in {"train", "test"}, "mode must be 'train' or 'test'"
        random.seed(seed)
        self.transform = transform
        self.noise_factor = noise_factor
        self.cache_dir = Path(cache_dir)
        self.mode = mode
        #self.add_noise = NoisyImageMixin()
        #self.epoch_number = self.add_noise.set_epoch(current_epoch, total_epochs=100
        self.add_noise = NoisyImageMixin(current_epoch , total_epoch)



        # Load selected image paths for this split
        self.image_paths = []
        with open(selected_file, "r") as f:
            for line in f:
                split, path = line.strip().split(maxsplit=1)
                if split == mode:
                    self.image_paths.append(Path(path))

        if not self.image_paths:
            raise ValueError(f"No images found for mode '{mode}' in {selected_file}")


    # def set_training_epoch(self, epoch):

    #     return self.add_noise.set_epoch(epoch)



    # def add_noise(self, img):
    #     noise_type = random.choice(["gaussian", "salt_pepper", "speckle"])
    #     if noise_type == "gaussian":
    #         noise = self.noise_factor * torch.randn_like(img)
    #         return torch.clamp(img + noise, 0., 1.)
    #     elif noise_type == "salt_pepper":
    #         noisy = img.clone()
    #         c, h, w = noisy.shape
    #         num_salt = int(0.05 * h * w * 0.5)
    #         num_pepper = int(0.05 * h * w * 0.5)
    #         for _ in range(num_salt):
    #             i, j = random.randint(0, h - 1), random.randint(0, w - 1)
    #             noisy[:, i, j] = 1.0
    #         for _ in range(num_pepper):
    #             i, j = random.randint(0, h - 1), random.randint(0, w - 1)
    #             noisy[:, i, j] = 0.0
    #         return noisy
    #     elif noise_type == "speckle":
    #         noise = self.noise_factor * torch.randn_like(img)
    #         return torch.clamp(img + img * noise, 0., 1.)

    # def apply_random_noise(self, img, allow_clean=True, clean_prob=0.1):

    def set_epochs(epoch , total_epoch = 100):
        self.add_noise.set_epoch(epoch , total_epoch)


    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        img = Image.open(img_path).convert("RGB")
        if self.transform:
            img = self.transform(img)
        clean_img = img.clone()
        #noisy_img = self.add_noise(clean_img)
        noisy_img , noise_info = self.add_noise.apply_random_noise( clean_img , allow_clean=True, clean_prob=0.1):
        # Cache file assumed to have same base name as image but with .pt extension
        #feat_path = self.cache_dir / img_path.name.replace(".jpg", ".pt")
        relative_path = img_path.relative_to(img_path.parents[2])
        feat_path = self.cache_dir / relative_path.with_suffix(".pt")
        clean_feat = torch.load(feat_path)

        return noisy_img, clean_img, clean_feat , noise_info

class MyCustomDenoisingDataset(torch.utils.data.Dataset, NoisyImageMixin):
    def __init__(self, ..., total_epochs=100):
        super().__init__()
        self.set_epoch(0, total_epochs)
        ...

    def set_training_epoch(self, epoch):
        self.set_epoch(epoch)

    def __getitem__(self, idx):
        clean_img = self.load_clean_image(idx)
        noisy_img, noise_info = self.apply_random_noise(clean_img)
        return noisy_img, clean_img, noise_info

transform = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.ToTensor(),
])

train_dataset = StratifiedFeatureDataset(
    selected_file="selected_images.txt",
    mode="train",
    cache_dir="cached_features",
    transform=transform
)

test_dataset = StratifiedFeatureDataset(
    selected_file="selected_images.txt",
    mode="test",
    cache_dir="cached_features",
    transform=transform
)

import torch
from torch.utils.data import Dataset
from PIL import Image
import pathlib

class CelebADataset(Dataset):
    def __init__(self, image_dir, transform=None):
        self.image_paths = sorted(list(pathlib.Path(image_dir).rglob("*.jpg")))
        if not self.image_paths :
          raise ValueError(f"NO image found in path .")
        self.transform = transform

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        if idx >= len(self.image_paths) :
          raise IndexError(f"Idex {idx} is out of bounds for dataset with length{len(self.image_paths)}")
        image = Image.open(self.image_paths[idx]).convert("RGB")
        if self.transform:
            image = self.transform(image)
        return image

# Define your custom datasets
# dataset1 = CustomDataset(data1, labels1, transform)
# dataset2 = CustomDataset(data2, labels2, transform)

# # Combine them
# combined_dataset = ConcatDataset([dataset1, dataset2])

# # Create DataLoader with shuffle
#dataloader = DataLoader(combined_dataset, batch_size=32, shuffle=True, num_workers=4)

"""# New Dataset for caching clean data (helps with Efficiency)"""

class LazyCachedDataset(Dataset):
    def __init__(self, noisy_paths, clean_paths, feature_dir, transform=None):
        self.noisy_paths = noisy_paths
        self.clean_paths = clean_paths
        self.feature_dir = feature_dir
        self.transform = transform

    def __getitem__(self, idx):
        noisy = Image.open(self.noisy_paths[idx]).convert("RGB")
        clean = Image.open(self.clean_paths[idx]).convert("RGB")

        if self.transform:
            noisy = self.transform(noisy)
            clean = self.transform(clean)

        img_name = os.path.basename(self.clean_paths[idx])
        feat_path = os.path.join(self.feature_dir, img_name.replace(".png", ".pt")) # helps with lazy loading
        clean_feat = torch.load(feat_path)

        return noisy, clean, clean_feat

Path("/content/ImageNet500/imagenet/sunscreen/299.jpg")

def Cache_Clean(dataset):
  for i, img in enumerate(clean_dataset):
    img = img.to(device)
    img = (img - mean) / std
    img = F.interpolate(img.unsqueeze(0), scale_factor=0.5, mode='bilinear', align_corners=False)
    features = model.features(img)
    torch.save(features.cpu(), f'cached_features/{i}.pt')



"""# start caching from here"""

import torch
from torch.utils.data import Dataset
class CelebADatasetSubset(Dataset):
    def __init__(self, selected_files_to_cache , session_mode , transform = None):
      assert session_mode in {"train" , "test"} , "session_mode must be either train or test ."
      self.transform = transform or transforms.ToTensor()
      self.session_mode = session_mode
      self.image_paths = []
      with open(selected_files_to_cache , "r") as f:
          for line in f:
              split, path = line.strip().split(maxsplit=1)
              if split == session_mode:
                  self.image_paths.append(Path(path))

      if not self.image_paths:
          raise ValueError(f"No images found for mode '{session_mode}' in {selected_files_to_cache}")

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        img = Image.open(img_path).convert("RGB")
        clean = self.transform(img)
        #noisy = clean + self.noise_factor * torch.randn_like(clean)
        #noisy = torch.clamp(noisy, 0.0, 1.0)

        # if self.cache:
        #     feature = self.cache(clean, img_path.name)
        #     return noisy, clean, feature
        # else:
        #     return noisy, clean
        return clean , img_path



selected_images = Path("/content/drive/MyDrive/ImageNet500k/selected_images.txt")

with open( selected_images, "r") as f:
          for line in f:
              split, path = line.strip().split(maxsplit=1)
              path
              break

type(path)

from pathlib import Pa

type(path)

path

path3 = Path(path)
path3

path4 = Path(path.strip().split(maxsplit = 1))
path4

path.strip()

path7 = "train /content/ImageNet500/imagenet/paddlewheel/489.jpg"

path5 =path7.strip().split(maxsplit = 1)
path5

path5[1]

Path(path5[1]).name

with open(selected_txt, "r") as f:
            self.selected_names = set(Path(line.strip().split(maxsplit=1)[1]).name for line in f)

        self.mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)
        self.std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device)

    def forward(self):
        for img, path in self.dataset:
            if path.name not in self.selected_names:
                continue



import torch
import torch.nn as nn
import torch.nn.functional as F
import pathlib
from PIL import Image
class Cache(nn.Module):
    def __init__(self, dataset, feature_extractor, selected_txt, save_dir, device, scale_factor=0.5):
        super().__init__()
        self.dataset = dataset
        self.feature_extractor = feature_extractor.to(device).eval()
        self.save_dir = pathlib.Path(save_dir)
        self.device = device
        self.scale_factor = scale_factor

        # Load only file names from the selected file
        # with open(selected_txt, "r") as f:
        #     self.selected_names = set(Path(line.strip().split(maxsplit=1)[1]).name for line in f)

        self.mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)
        self.std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device)

    def forward(self):
        for img, path in self.dataset:
            # if path.name not in self.selected_names:
            #     continue

            #save_path = self.save_dir / f"{path.stem}.pt"
            # Preserve class structure:
            relative_path = path.relative_to(path.parents[2])  # e.g., imagenet/dog/123.jpg
            save_path = self.save_dir / relative_path.with_suffix(".pt")
            save_path.parent.mkdir(parents=True, exist_ok=True)

            if save_path.exists():
                continue  # skip if already cached

            img = img.to(self.device).unsqueeze(0)
            img = (img - self.mean) / self.std
            img = F.interpolate(img, scale_factor=self.scale_factor, mode="bilinear", align_corners=False)

            with torch.no_grad():
                features = self.feature_extractor(img)

            torch.save(features.cpu(), save_path)

# Preserve class structure:
relative_path = path.relative_to(path.parents[2])  # e.g., imagenet/dog/123.jpg
save_path = self.save_dir / relative_path.with_suffix(".pt")
save_path.parent.mkdir(parents=True, exist_ok=True)

path = Path("/content/cache_dir")

r = p.relative_to(p.parents[2])
s = path / r.with_suffix(".pt")
s

r



s.parent.mkdir(parents = True , exist_ok = True )

s.mkdir(parents = True , exist_ok = True )

p = Path(path5[1])
p

p.stem

with open("selected_images.txt" , 'r' ) as f :
  selected_images = set(Path(line.strip().split(maxsplit=1)[1]).name for line in f)

len(selected_images)

line = "train /content/ImageNet500/imagenet/sunscreen/363.jpg"
line.strip()

line_striped = line.strip()

striped = line_striped.split(maxsplit = 1)[1]
striped

striped_path = Path(strip)







import torchvision.transforms as transforms
transform_2 = transforms.Compose([
    #transforms.Resize((256 , 256)),
    #transforms.Grayscale(num_output_channels = 3),
    transforms.ToTensor()
])

dataset_test = CelebADatasetSubset("selected_images.txt", session_mode="test", transform=transform_2)

len(dataset_test)

device = "cuda" if torch.cuda.is_available() else "cpu"
device

import torchvision.models as models
import torch.nn as nn

class ResNet18Perceptual(nn.Module):
    def __init__(self):
        super().__init__()
        resnet = models.resnet18(weights='IMAGENET1K_V1')  # or pretrained=True in older versions
        self.features = nn.Sequential(
            resnet.conv1,
            resnet.bn1,
            resnet.relu,
            resnet.maxpool,
            resnet.layer1,
            #resnet.layer2,  # 👈 we stop here
        )
        for p in self.features.parameters():
            p.requires_grad = False

    def forward(self, x):
        return self.features(x)

fe = ResNet18Perceptual()

import time
start = time.time()
cache = Cache(
    dataset=dataset_test,
    feature_extractor=fe,
    selected_txt="selected_images.txt",
    save_dir="/content/Cache",
    device=device,
    scale_factor=0.5
)

cache.forward()
print(f" the time of running : {time.time() - start}")

randompic = Path("/content/Cache/883.pt")
a = torch.load(randompic)

num_features = 8
fig, axes = plt.subplots(1, num_features, figsize=(20, 5))
for idx in range(num_features):
    axes[idx].imshow(features[idx].cpu().detach().numpy(), cmap='viridis')
    axes[idx].axis('off')
plt.show()

img = (img - img.min()) / (img.max() - img.min())  # normalize between 0 and 1
axes[idx].imshow(img, cmap='viridis')

import matplotlib.pyplot as plt

a = a.squeeze(0)

import torch
import matplotlib.pyplot as plt

# Suppose this is your feature map
# = torch.randn(1, 64, 32, 32)  # or your actual feature

# Remove batch dimension
feature_map = a# shape: [64, 32, 32]

# Plot first 16 channels in a 4x4 grid
plt.figure(figsize=(10, 10))
for i in range(16):
    plt.subplot(4, 4, i + 1)
    plt.imshow(feature_map[i].cpu().detach().numpy(), cmap='viridis')
    plt.axis('off')
    plt.title(f'Channel {i}')
plt.tight_layout()
plt.show()

a.shape

cached_features = Path("/content/Cache")
os.makedirs(cached_features , exist_ok= True )

from torchvision import models, transforms

# Setup transform, model, device
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor()
])

model = models.resnet18(pretrained=True).eval()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Prepare dataset
dataset = simple_dataset("selected_images.txt", session_mode="train", transform=transform)

# Initialize and run cache
cache = Cache(
    dataset=dataset,
    feature_extractor=model,
    selected_txt="selected_images.txt",
    save_dir="cached_features",
    device=device,
    scale_factor=0.5
)

cache.forward()

import random
import matplotlib.pyplot as plt
def plot_transformed_images(image_paths, transform, n=3, seed=42):
    """Plots a series of random images from image_paths.

    Will open n image paths from image_paths, transform them
    with transform and plot them side by side.

    Args:
        image_paths (list): List of target image paths.
        transform (PyTorch Transforms): Transforms to apply to images.
        n (int, optional): Number of images to plot. Defaults to 3.
        seed (int, optional): Random seed for the random generator. Defaults to 42.
    """
    random.seed(seed)
    random_image_paths = random.sample(image_paths, k=n)
    for image_path in random_image_paths:
        with Image.open(image_path) as f:
            fig, ax = plt.subplots(1, 2 , figsize = (6,6))
            ax[0].imshow(f)
            ax[0].set_title(f"Original \nSize: {f.size}")
            ax[0].axis("off")


            transformed_image = transform(f).permute(1, 2, 0)
            ax[1].imshow(transformed_image)
            ax[1].set_title(f"Transformed \nSize: {transformed_image.shape}")
            ax[1].axis("off")

            #fig.suptitle(f"Class: {image_path.parent.stem}", fontsize=16)

image_path_list = list(dataset_Image.rglob("*.jpg"))
plot_transformed_images(image_path_list,
                        transform=transform_2,
                        n=3)

from torchvision.datasets import MNIST
from torch.utils.data import Dataset
import torchvision.transforms as transforms
import numpy as np
import torch

class NoisyCelebA(Dataset):
    def __init__(self, seed ,img_dir , noise_factor=0.25 , transform = None ) :
        torch.manual_seed(seed)
        self.data = dataset = CelebADataset(img_dir , transform=transform)
        self.noise_factor = noise_factor
        #self.classes = self.data.classe


    def add_gaussian_noise(self, img, noise_factor=0.25):
        noise = noise_factor * torch.randn_like(img)
        return torch.clamp(img + noise, 0., 1.)

    def add_salt_and_pepper_noise(self, img, amount=0.05, s_vs_p=0.5):
        noisy = img.clone()
        c, h, w = noisy.shape
        num_salt = int(amount * h * w * s_vs_p)
        num_pepper = int(amount * h * w * (1 - s_vs_p))

        # Salt (white) noise
        for _ in range(num_salt):
            i = random.randint(0, h - 1)
            j = random.randint(0, w - 1)
            noisy[:, i, j] = 1.0

        # Pepper (black) noise
        for _ in range(num_pepper):
            i = random.randint(0, h - 1)
            j = random.randint(0, w - 1)
            noisy[:, i, j] = 0.0

        return noisy

    def add_speckle_noise(self, img, noise_factor=0.25):
        noise = noise_factor * torch.randn_like(img)
        return torch.clamp(img + img * noise, 0., 1.)

    def __getitem__(self, index):
        clean_img = self.data[index]

        noise_type = random.choice(['gaussian', 'salt_pepper', 'speckle'])

        if noise_type == 'gaussian':
            noisy_img = self.add_gaussian_noise(clean_img)
        elif noise_type == 'salt_pepper':
            noisy_img = self.add_salt_and_pepper_noise(clean_img)
        else:
            noisy_img = self.add_speckle_noise(clean_img)

        return noisy_img, clean_img

    def __len__(self):
        return len(self.data)







import torch
from torch.utils.data import Dataset
from PIL import Image
import pathlib

class CelebADataset(Dataset):
    def __init__(self, image_dir, feature_dir , noise_factor, seed , transform=None):
        torch.manual_seed(seed = seed)
        self.image_paths = sorted(list(pathlib.Path(image_dir).rglob("*.jpg")))
        if not self.image_paths :
          raise ValueError(f"NO image found in path .")
        self.noise_factor = noise_factor
        self.transform = transform


    def add_gaussian_noise(self, img, noise_factor=0.25):
        noise = noise_factor * torch.randn_like(img)
        return torch.clamp(img + noise, 0., 1.)

    def add_salt_and_pepper_noise(self, img, amount=0.05, s_vs_p=0.5):
        noisy = img.clone()
        c, h, w = noisy.shape
        num_salt = int(amount * h * w * s_vs_p)
        num_pepper = int(amount * h * w * (1 - s_vs_p))

        # Salt (white) noise
        for _ in range(num_salt):
            i = random.randint(0, h - 1)
            j = random.randint(0, w - 1)
            noisy[:, i, j] = 1.0

        # Pepper (black) noise
        for _ in range(num_pepper):
            i = random.randint(0, h - 1)
            j = random.randint(0, w - 1)
            noisy[:, i, j] = 0.0

        return noisy

    def add_speckle_noise(self, img, noise_factor=0.25):
        noise = noise_factor * torch.randn_like(img)
        return torch.clamp(img + img * noise, 0., 1.)



    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        if idx >= len(self.image_paths) :
          raise IndexError(f"Idex {idx} is out of bounds for dataset with length{len(self.image_paths)}")
        image = Image.open(self.image_paths[idx]).convert("RGB")
        #clean_feature = Image.open(self.cache_features[idx]) # do we need conveting to RGB in here ?
        if self.transform:
            image = self.transform(image)
        clean_img = image[idx]


        img_name = os.path.basename(self.image_paths[idx])
        feat_path = os.path.join(self.cache_features, img_name.replace(".png", ".pt")) # helps with lazy loading
        clean_feat = torch.load(feat_path)

        noise_type = random.choice(['gaussian', 'salt_pepper', 'speckle'])

        if noise_type == 'gaussian':
            noisy_img = self.add_gaussian_noise(clean_img)
        elif noise_type == 'salt_pepper':
            noisy_img = self.add_salt_and_pepper_noise(clean_img)
        else:
            noisy_img = self.add_speckle_noise(clean_img)

        return noisy_img, clean_img , clean_feat



import torch
import random

class NoisyImageMixin:
    def add_gaussian_noise(self, img, noise_factor=0.25):
        noise = noise_factor * torch.randn_like(img)
        return torch.clamp(img + noise, 0., 1.)

    def add_salt_and_pepper_noise(self, img, amount=0.05, s_vs_p=0.5):
        noisy = img.clone()
        c, h, w = noisy.shape
        num_salt = int(amount * h * w * s_vs_p)
        num_pepper = int(amount * h * w * (1 - s_vs_p))

        for _ in range(num_salt):
            i = random.randint(0, h - 1)
            j = random.randint(0, w - 1)
            noisy[:, i, j] = 1.0

        for _ in range(num_pepper):
            i = random.randint(0, h - 1)
            j = random.randint(0, w - 1)
            noisy[:, i, j] = 0.0

        return noisy

    def add_speckle_noise(self, img, noise_factor=0.25):
        noise = noise_factor * torch.randn_like(img)
        return torch.clamp(img + img * noise, 0., 1.)

    def apply_random_noise(self, img):
        noise_type = random.choice(['gaussian', 'salt_pepper', 'speckle'])
        if noise_type == 'gaussian':
            return self.add_gaussian_noise(img)
        elif noise_type == 'salt_pepper':
            return self.add_salt_and_pepper_noise(img)
        else:
            return self.add_speckle_noise(img)





import torch
import random

class NoisyImageMixin:
    def add_gaussian_noise(self, img, noise_factor=None):
        if noise_factor is None:
            noise_factor = random.uniform(0.05, 0.5)  # adjustable range
        noise = noise_factor * torch.randn_like(img)
        return torch.clamp(img + noise, 0., 1.)

    def add_salt_and_pepper_noise(self, img, amount=None, s_vs_p=None):
        if amount is None:
            amount = random.uniform(0.01, 0.1)  # proportion of pixels affected
        if s_vs_p is None:
            s_vs_p = random.uniform(0.3, 0.7)  # salt-to-pepper ratio

        noisy = img.clone()
        c, h, w = noisy.shape
        num_salt = int(amount * h * w * s_vs_p)
        num_pepper = int(amount * h * w * (1 - s_vs_p))

        for _ in range(num_salt):
            i = random.randint(0, h - 1)
            j = random.randint(0, w - 1)
            noisy[:, i, j] = 1.0

        for _ in range(num_pepper):
            i = random.randint(0, h - 1)
            j = random.randint(0, w - 1)
            noisy[:, i, j] = 0.0

        return noisy

    def add_speckle_noise(self, img, noise_factor=None):
        if noise_factor is None:
            noise_factor = random.uniform(0.05, 0.4)
        noise = noise_factor * torch.randn_like(img)
        return torch.clamp(img + img * noise, 0., 1.)

    def apply_random_noise(self, img, allow_clean=True, clean_prob=0.1):
        """
        Applies random noise or returns clean image with a certain probability.

        Args:
            img: Tensor image (C, H, W)
            allow_clean: Whether to sometimes return clean images
            clean_prob: Probability to return clean image (float in [0,1])
        """
        if allow_clean and random.random() < clean_prob:
            return img.clone()

        # if random.random() < 0.2:
        #     noisy_img = self.pde_denoise(self.add_gaussian_noise(clean_img))       #if and only if we had something shitty to idk ...
                                                                                          #just kidding i mean if we had pde based preprocessed data

        noise_type = random.choice(['gaussian', 'salt_pepper', 'speckle'])
        if noise_type == 'gaussian':
            return self.add_gaussian_noise(img)
        elif noise_type == 'salt_pepper':
            return self.add_salt_and_pepper_noise(img)
        else:
            return self.add_speckle_noise(img)





import torch
import random

class NoisyImageMixin:
    def add_gaussian_noise(self, img, noise_factor=None):
        if noise_factor is None:
            noise_factor = random.uniform(0.05, 0.5)
        noise = noise_factor * torch.randn_like(img)
        noisy = torch.clamp(img + noise, 0., 1.)
        return noisy, {'type': 'gaussian', 'factor': noise_factor}

    def add_salt_and_pepper_noise(self, img, amount=None, s_vs_p=None):
        if amount is None:
            amount = random.uniform(0.01, 0.1)
        if s_vs_p is None:
            s_vs_p = random.uniform(0.3, 0.7)

        noisy = img.clone()
        c, h, w = noisy.shape
        num_salt = int(amount * h * w * s_vs_p)
        num_pepper = int(amount * h * w * (1 - s_vs_p))

        for _ in range(num_salt):
            i = random.randint(0, h - 1)
            j = random.randint(0, w - 1)
            noisy[:, i, j] = 1.0

        for _ in range(num_pepper):
            i = random.randint(0, h - 1)
            j = random.randint(0, w - 1)
            noisy[:, i, j] = 0.0

        return noisy, {'type': 'salt_pepper', 'amount': amount, 's_vs_p': s_vs_p}

    def add_speckle_noise(self, img, noise_factor=None):
        if noise_factor is None:
            noise_factor = random.uniform(0.05, 0.4)
        noise = noise_factor * torch.randn_like(img)
        noisy = torch.clamp(img + img * noise, 0., 1.)
        return noisy, {'type': 'speckle', 'factor': noise_factor}

    def apply_random_noise(self, img, allow_clean=True, clean_prob=0.1):
        if allow_clean and random.random() < clean_prob:
            return img.clone(), {'type': 'clean'}

                # if random.random() < 0.2:
        #     noisy_img = self.pde_denoise(self.add_gaussian_noise(clean_img))       #if and only if we had something shitty to idk ...
        # return noisy_img , {'type' : 'pde denoised' }                                                                          #just kiddin

        noise_type = random.choice(['gaussian', 'salt_pepper', 'speckle'])
        if noise_type == 'gaussian':
            return self.add_gaussian_noise(img)
        elif noise_type == 'salt_pepper':
            return self.add_salt_and_pepper_noise(img)
        else:
            return self.add_speckle_noise(img)



class MyDenoisingDataset(Dataset, NoisyImageMixin):
    def __init__(self, image_list, transform=None):
        self.image_list = image_list
        self.transform = transform

    def __getitem__(self, idx):
        clean_img = self.image_list[idx]
        if self.transform:
            clean_img = self.transform(clean_img)

        noisy_img, noise_info = self.apply_random_noise(clean_img, allow_clean=True, clean_prob=0.1)

        return noisy_img, clean_img, noise_info



import torchvision.transforms as T

train_transform = T.Compose([
    T.RandomHorizontalFlip(p=0.5),
    T.RandomRotation(degrees=10),
    T.RandomAffine(degrees=5, translate=(0.01, 0.01)),
])



import torch
from torch.utils.data import Dataset
from PIL import Image
import pathlib
import os
from torchvision import transforms
import random
#from your_module import NoisyImageMixin  # or paste the mixin directly above

class CelebADataset(Dataset, NoisyImageMixin):
    def __init__(self, image_dir, feature_dir, noise_factor, seed, transform=None):
        torch.manual_seed(seed)
        self.image_paths = sorted(list(pathlib.Path(image_dir).rglob("*.jpg")))
        if not self.image_paths:
            raise ValueError(f"No images found in path: {image_dir}")

        self.feat_paths = [
            os.path.join(feature_dir, os.path.basename(p).replace(".jpg", ".pt"))
            for p in self.image_paths
        ]

        self.noise_factor = noise_factor
        self.transform = transform

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        image_path = self.image_paths[idx]
        feat_path = self.feat_paths[idx]

        image = Image.open(image_path).convert("RGB")
        if self.transform:
            clean_img = self.transform(image)
        else:
            clean_img = transforms.ToTensor()(image)

        clean_feat = torch.load(feat_path)

        noisy_img = self.apply_random_noise(clean_img)

        return noisy_img, clean_img, clean_feat









DataSet = NoisyCelebA(seed=42, img_dir= "/content/ImageNet500/imagenet",
                         noise_factor=0.25, transform=transform_2)



def plot_custom_dataset(dataset , n):
  for datasample in range(n):
    sample_0 = dataset[datasample]
    noisy , clean = sample_0
    fig, ax = plt.subplots(1, 2 , figsize = (6,6))
    noisy_image = noisy.permute(1,2,0)
    ax[0].imshow(noisy_image)
    ax[0].set_title(f"Original \nSize: {noisy_image.shape}")
    ax[0].axis("off")


    clean_image = clean.permute(1, 2, 0)
    ax[1].imshow(clean_image)
    ax[1].set_title(f"Transformed \nSize: {clean_image.shape}")
    ax[1].axis("off")

plot_custom_dataset(DataSet , 3)

len(DataSet)

from torch.utils.data import Subset
import numpy as np
def split_dataset(dataset , train_number , test_number , seed ):
  indices = np.arange(len(dataset))
  np.random.seed(seed)  # for reproducibility
  np.random.shuffle(indices)

  train_indices = indices[:train_number]
  test_indices = indices[train_number : train_number + test_number]  # for example, 5000 test

  train_data = Subset(dataset, train_indices)
  test_data = Subset(dataset, test_indices)

  return train_data , test_data




data_train , data_test = split_dataset(DataSet , 30000 , 5000 , seed = 42 )
len(data_train) , len(data_test)

NUM_WORKERS = 2 if os.cpu_count() <= 4 else 4
NUM_WORKERS

from torch.utils.data import DataLoader
train_dataloader = DataLoader(data_train , batch_size = 16, shuffle = True , num_workers = NUM_WORKERS , pin_memory = True )
test_dataloader = DataLoader(data_test , batch_size = 16 , shuffle= True , num_workers = NUM_WORKERS , pin_memory = True )

device = "cuda" if torch.cuda.is_available() else "cpu"
device

#torch.cuda.empty_cache()

import torch
import torch.nn as nn

class DenoisingAutoencoder(nn.Module):
    def __init__(self):
        super(DenoisingAutoencoder, self ).__init__()

        # Encoder
        self.encoder = nn.Sequential(
            nn.Conv2d(3 , 64, kernel_size=3, stride=1, padding=1),  # Input size: 256x256x3
            nn.ReLU(),
            nn.MaxPool2d(2, 2),  # Output size: 128x128x64

            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),  # Output size: 64x64x128

            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),  # Output size: 32x32x256

            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),  # Output size: 16x16x512
        )

        # Decoder
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.ReLU(),  # Output size: 32x32x256

            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.ReLU(),  # Output size: 64x64x128

            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.ReLU(),  # Output size: 128x128x64

            nn.ConvTranspose2d(64, 3 , kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.Sigmoid()  # Output size: 256x256x3
        )

    def forward(self, x):
      if x is None:
        raise ValueError("Input tensor is None!")
      #print(f"[Forward] Input shape: {x.shape}")
      x = self.encoder(x)
      x = self.decoder(x)
      return x

# Initialize the model
torch.manual_seed(42)
model_denoise_2  = DenoisingAutoencoder().to(device)

model_denoise_2

# Print a summary using torchinfo (uncomment for actual output)
from torchinfo import summary
summary(model= model_denoise_2,
        input_size=(64, 3, 256, 256), # make sure this is "input_size", not "input_shape"
        # col_names=["input_size"], # uncomment for smaller output
        col_names=["input_size", "output_size", "num_params", "trainable"],
        col_width=20,
        row_settings=["var_names"]
)

import torch
import torch.nn as nn
class SEBlock(nn.Module):
    def __init__(self, channels, reduction=16):
        super().__init__()
        self.pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Linear(channels, channels // reduction),
            nn.ReLU(inplace=True),
            nn.Linear(channels // reduction, channels),
            nn.Sigmoid()
        )

    def forward(self, x):
        b, c, _, _ = x.size()
        y = self.pool(x).view(b, c)
        y = self.fc(y).view(b, c, 1, 1)
        return x * y


class AttentionUNetDenoiser(nn.Module):
    def __init__(self):
        super().__init__()

        def conv_block(in_ch, out_ch):
            return nn.Sequential(
                nn.Conv2d(in_ch, out_ch, 3, padding=1),
                nn.ReLU(inplace=True),
                nn.Conv2d(out_ch, out_ch, 3, padding=1),
                nn.ReLU(inplace=True),
                SEBlock(out_ch)  # <--- add SE attention here
            )

        self.enc1 = conv_block(3, 64)
        self.enc2 = conv_block(64, 128)
        self.enc3 = conv_block(128, 256)

        self.pool = nn.MaxPool2d(2)
        self.middle = conv_block(256, 512)

        self.up3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)
        self.dec3 = conv_block(512, 256)

        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)
        self.dec2 = conv_block(256, 128)

        self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)
        self.dec1 = conv_block(128, 64)

        self.out = nn.Conv2d(64, 3, kernel_size=1)
        self.final = nn.Sigmoid()

    def forward(self, x):
        e1 = self.enc1(x)
        e2 = self.enc2(self.pool(e1))
        e3 = self.enc3(self.pool(e2))

        m = self.middle(self.pool(e3))

        d3 = self.dec3(torch.cat([self.up3(m), e3], dim=1))
        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))
        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))

        return self.final(self.out(d1))


attention_model = AttentionUNetDenoiser().to(device)
attention_model

!pip install torchinfo
from torchinfo import summary

# Print a summary using torchinfo (uncomment for actual output)
from torchinfo import summary
summary(model= attention_model,
        input_size=(64, 3, 256, 256), # make sure this is "input_size", not "input_shape"
        # col_names=["input_size"], # uncomment for smaller output
        col_names=["input_size", "output_size", "num_params", "trainable"],
        col_width=20,
        row_settings=["var_names"]
)

#skipp conections

import torch.nn as nn
import torch

class UNetDenoiser(nn.Module):
    def __init__(self):
        super().__init__()

        def conv_block(in_ch, out_ch):
            return nn.Sequential(
                nn.Conv2d(in_ch, out_ch, 3, padding=1),
                nn.ReLU(inplace=True),
                nn.Conv2d(out_ch, out_ch, 3, padding=1),
                nn.ReLU(inplace=True)
            )

        self.enc1 = conv_block(3, 64)
        self.enc2 = conv_block(64, 128)
        self.enc3 = conv_block(128, 256)

        self.pool = nn.MaxPool2d(2)

        self.middle = conv_block(256, 512)

        self.up3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)
        self.dec3 = conv_block(512, 256)

        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)
        self.dec2 = conv_block(256, 128)

        self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)
        self.dec1 = conv_block(128, 64)

        self.out = nn.Conv2d(64, 3, kernel_size=1)
        self.final = nn.Sigmoid()

    def forward(self, x):
        e1 = self.enc1(x)  # 64x256x256
        e2 = self.enc2(self.pool(e1))  # 128x128x128
        e3 = self.enc3(self.pool(e2))  # 256x64x64

        m = self.middle(self.pool(e3))  # 512x32x32

        d3 = self.up3(m)  # 256x64x64
        d3 = self.dec3(torch.cat([d3, e3], dim=1))  # Skip connection

        d2 = self.up2(d3)  # 128x128x128
        d2 = self.dec2(torch.cat([d2, e2], dim=1))

        d1 = self.up1(d2)  # 64x256x256
        d1 = self.dec1(torch.cat([d1, e1], dim=1))

        out = self.final(self.out(d1))
        return out

Unet_skip_model = UNetDenoiser().to(device)
Unet_skip_model

# Print a summary using torchinfo (uncomment for actual output)
from torchinfo import summary
summary(model= Unet_skip_model,
        input_size=(64, 3, 256, 256), # make sure this is "input_size", not "input_shape"
        # col_names=["input_size"], # uncomment for smaller output
        col_names=["input_size", "output_size", "num_params", "trainable"],
        col_width=20,
        row_settings=["var_names"]
)

import torch.nn as nn

class EnhancedDenoiser(nn.Module):
    def __init__(self):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 64, 3, padding=1), nn.ReLU(),
            nn.Conv2d(64, 64, 3, padding=1), nn.ReLU(),
            nn.Conv2d(64, 128, 3, stride=2, padding=1), nn.ReLU(),

            nn.Conv2d(128, 128, 3, padding=1), nn.ReLU(),
            nn.Conv2d(128, 256, 3, stride=2, padding=1), nn.ReLU()
        )

        self.middle = nn.Sequential(
            nn.Conv2d(256, 256, 3, padding=1), nn.ReLU(),
            nn.Conv2d(256, 256, 3, padding=1), nn.ReLU()
        )

        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1), nn.ReLU(),
            nn.Conv2d(128, 128, 3, padding=1), nn.ReLU(),

            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1), nn.ReLU(),
            nn.Conv2d(64, 64, 3, padding=1), nn.ReLU(),
            nn.Conv2d(64, 3, 3, padding=1), nn.Sigmoid()
        )

    def forward(self, x):
        x = self.encoder(x)
        x = self.middle(x)
        x = self.decoder(x)
        return x

enhanced_model = EnhancedDenoiser().to(device)
enhanced_model

# Print a summary using torchinfo (uncomment for actual output)
from torchinfo import summary
summary(model= enhanced_model,
        input_size=(64, 3, 256, 256), # make sure this is "input_size", not "input_shape"
        # col_names=["input_size"], # uncomment for smaller output
        col_names=["input_size", "output_size", "num_params", "trainable"],
        col_width=20,
        row_settings=["var_names"]
)

"""# Monitoring loss functions"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import models

class PerceptualVGG19Loss(nn.Module):
    def __init__(self, device='cpu'):
        super().__init__()
        vgg = models.vgg19(pretrained=True).features[:36]  # [:36] includes relu4_4
          # relu2_2 ends at layer 9
        self.vgg = vgg.eval().to(device)
        for param in self.vgg.parameters():
            param.requires_grad = False

        self.normalize = nn.Sequential(
            nn.Conv2d(3, 3, kernel_size=1),  # Dummy layer to register mean/std buffers
        )
        mean = torch.tensor([0.485, 0.456, 0.406]).view(1,3,1,1)
        std = torch.tensor([0.229, 0.224, 0.225]).view(1,3,1,1)
        self.register_buffer('mean', mean)
        self.register_buffer('std', std)

        self.device = device

    def forward(self, x, y):
        # Normalize without modifying the original inputs
        x = x.to(self.device)
        y = y.to(self.device)

        x = (x - self.mean) / self.std
        y = (y - self.mean) / self.std

        return F.l1_loss(self.vgg(x), self.vgg(y)).to(device)


class GradientLoss(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x, y):
        dx = torch.abs(x[:, :, :, :-1] - x[:, :, :, 1:])
        dy = torch.abs(x[:, :, :-1, :] - x[:, :, 1:, :])
        dx_gt = torch.abs(y[:, :, :, :-1] - y[:, :, :, 1:])
        dy_gt = torch.abs(y[:, :, :-1, :] - y[:, :, 1:, :])
        return F.l1_loss(dx, dx_gt) + F.l1_loss(dy, dy_gt)


class CombinedLoss(nn.Module):
    def __init__(self, device='cpu', lambda_vgg=0.1, lambda_grad=0.2):
        super().__init__()
        self.l1 = nn.L1Loss()
        self.vgg = PerceptualVGG19Loss(device)
        self.grad = GradientLoss()
        self.lambda_vgg = lambda_vgg
        self.lambda_grad = lambda_grad

    def forward(self, pred, target):
        l1 = self.l1(pred, target)
        vgg = self.vgg(pred, target)
        grad = self.grad(pred, target)
        total = l1 + self.lambda_vgg * vgg + self.lambda_grad * grad
        return total, {'l1': l1.item(), 'vgg': vgg.item(), 'grad': grad.item()}







import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import models

# class PerceptualVGG19Loss(nn.Module):
#     def __init__(self, device='cpu'):
#         super().__init__()
#         vgg = models.vgg19(pretrained=True).features[:36]  # [:36] includes relu4_4
#           # relu2_2 ends at layer 9
#         self.vgg = vgg.eval().to(device)
#         for param in self.vgg.parameters():
#             param.requires_grad = False

#         self.normalize = nn.Sequential(
#             nn.Conv2d(3, 3, kernel_size=1),  # Dummy layer to register mean/std buffers
#         )
#         mean = torch.tensor([0.485, 0.456, 0.406]).view(1,3,1,1)
#         std = torch.tensor([0.229, 0.224, 0.225]).view(1,3,1,1)
#         self.register_buffer('mean', mean)
#         self.register_buffer('std', std)

#         self.device = device

#     def forward(self, x, y):
#         # Normalize without modifying the original inputs
#         x = x.to(self.device)
#         y = y.to(self.device)

#         x = (x - self.mean) / self.std
#         y = (y - self.mean) / self.std

#         return F.l1_loss(self.vgg(x), self.vgg(y)).to(device)


class GradientLoss(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x, y):
        dx = torch.abs(x[:, :, :, :-1] - x[:, :, :, 1:])
        dy = torch.abs(x[:, :, :-1, :] - x[:, :, 1:, :])
        dx_gt = torch.abs(y[:, :, :, :-1] - y[:, :, :, 1:])
        dy_gt = torch.abs(y[:, :, :-1, :] - y[:, :, 1:, :])
        return F.l1_loss(dx, dx_gt) + F.l1_loss(dy, dy_gt)


class CombinedLoss(nn.Module):
    def __init__(self, device='cpu', lambda_vgg=0.1, lambda_grad=0.2):
        super().__init__()
        self.l1 = nn.L1Loss()
        #self.vgg = PerceptualVGG19Loss(device)
        self.perceptual = ResNet18Perceptual('layer1' , device)
        self.grad = GradientLoss()
        self.lambda_vgg = lambda_vgg
        self.lambda_grad = lambda_grad

    def forward(self, pred, target , down_factor , downscale : bool = True , amp : bool = False):
        l1 = self.l1(pred, target)
        vgg = self.perceptual(pred, target , down_factor = 0.5 , downscale = True , amp = False)
        grad = self.grad(pred, target)
        total = l1 + self.lambda_vgg * vgg + self.lambda_grad * grad
        return total, {'l1': l1.item(), 'vgg': vgg.item(), 'grad': grad.item()}

total_loss = CombinedLoss()

import torch.nn.functional as F
noise , clean = next(iter(train_dataloader))

noise = noise.to(device)
clean = clean.to(device)

with torch.no_grad():
  pred = model_denoise_2(noise)


import time
start = time.time()

loss = total_loss(pred , clean , down_factor = 0.5 , downscale = True , amp = False)
print(f"vgg19 loss forward time : {time.time() - start } ")

















import torch.nn.functional as F
noise , clean = next(iter(train_dataloader))

noise = noise.to(device)
clean = clean.to(device)

with torch.no_grad():
  pred = model_denoise_2(noise)


import time
start = time.time()

loss = loss1(pred , clean , down_factor = 0.5 , downscale = True , amp = False)
print(f"vgg19 loss forward time : {time.time() - start } ")



import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import models

class PerceptualVGG16Loss(nn.Module):
    def __init__(self, device='cpu'):
        super().__init__()
        #vgg = models.vgg19(pretrained=True).features[:36]  # [:36] includes relu4_4
        vgg = models.vgg16(pretrained=True).features[:9]  # relu2_2 ends at layer 9
        self.vgg = vgg.eval().to(device)
        for param in self.vgg.parameters():
            param.requires_grad = False

        self.normalize = nn.Sequential(
            nn.Conv2d(3, 3, kernel_size=1),  # Dummy layer to register mean/std buffers
        )
        mean = torch.tensor([0.485, 0.456, 0.406]).view(1,3,1,1)
        std = torch.tensor([0.229, 0.224, 0.225]).view(1,3,1,1)
        self.register_buffer('mean', mean)
        self.register_buffer('std', std)

        self.device = device

    def forward(self, x, y):
        # Normalize without modifying the original inputs
        x = x.to(self.device)
        y = y.to(self.device)

        x = (x - self.mean) / self.std
        y = (y - self.mean) / self.std


        x_features = self.vgg(x)
        y_features = self.vgg(y)

        if x_features.dim() == 0 or y_features.dim() == 0:
            raise ValueError(f"Unexpected output dimensions from VGG19: x_features: {x_features.shape}, y_features: {y_features.shape}")

        return F.l1_loss(x_features , y_features).to(device)

        #eturn F.l1_loss(self.vgg(x), self.vgg(y)).to(device)


class GradientLoss(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x, y):
        dx = torch.abs(x[:, :, :, :-1] - x[:, :, :, 1:])
        dy = torch.abs(x[:, :, :-1, :] - x[:, :, 1:, :])
        dx_gt = torch.abs(y[:, :, :, :-1] - y[:, :, :, 1:])
        dy_gt = torch.abs(y[:, :, :-1, :] - y[:, :, 1:, :])
        return F.l1_loss(dx, dx_gt) + F.l1_loss(dy, dy_gt)


class CombinedLoss(nn.Module):
    def __init__(self, device='cpu', lambda_vgg=0.1, lambda_grad=0.2):
        super().__init__()
        self.l1 = nn.L1Loss()
        self.vgg = PerceptualVGG19Loss(device)
        self.grad = GradientLoss()
        self.lambda_vgg = lambda_vgg
        self.lambda_grad = lambda_grad

    def forward(self, pred, target):
        l1 = self.l1(pred, target)
        vgg = self.vgg(pred, target)
        grad = self.grad(pred, target)
        total = l1 + self.lambda_vgg * vgg + self.lambda_grad * grad
        return total, {'l1': l1.item(), 'vgg': vgg.item(), 'grad': grad.item()}

import torch
from torch import nn
class GradientLoss(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x, y):
        dx = torch.abs(x[:, :, :, :-1] - x[:, :, :, 1:])
        dy = torch.abs(x[:, :, :-1, :] - x[:, :, 1:, :])
        dx_gt = torch.abs(y[:, :, :, :-1] - y[:, :, :, 1:])
        dy_gt = torch.abs(y[:, :, :-1, :] - y[:, :, 1:, :])
        return F.l1_loss(dx, dx_gt) + F.l1_loss(dy, dy_gt)

GLoss = GradientLoss()

vgg16 = PerceptualVGG16Loss(device = device )

import torch.nn.functional as F
noise , clean = next(iter(train_dataloader))

noise = noise.to(device)
clean = clean.to(device)

with torch.no_grad():
  pred = model_denoise_2(noise)


import time
start = time.time()

loss = GLoss(pred , clean)
print(f"vgg19 loss forward time : {time.time() - start } ")

noise , clean = next(iter(train_dataloader))

noise = noise.to(device)
clean = clean.to(device)

with torch.no_grad():
  pred = model_denoise_2(noise)


import time
start = time.time()

loss = vgg16(pred , clean)
print(f"vgg19 loss forward time : {time.time() - start } ")

l1_losss = nn.L1Loss()

!pip install lpips
import lpips

class LPIPSLoss(nn.Module):
    def __init__(self):
        super(LPIPSLoss, self).__init__()
        # LPIPS model
        self.lpips = lpips.LPIPS(net='vgg').eval()  # You can change to 'alex' or 'squeezenet'

    def forward(self, x, y):
        return self.lpips(x, y).mean()  # LPIPS returns a tensor, get the mean for a scalar loss


lpips = LPIPSLoss()

noise , clean = next(iter(train_dataloader))

noise = noise.to(device)
clean = clean.to(device)

with torch.no_grad():
  pred = model_denoise_2(noise)


import time
start = time.time()

loss = lpips(pred , clean)
print(f"vgg19 loss forward time : {time.time() - start } ")

len(train_dataloader)

from torchvision.models import vgg19, VGG19_Weights
self.vgg = vgg19(weights=VGG19_Weights.DEFAULT).features[:36]







import torch
import torch.nn as nn
import lpips

class LPIPSLoss(nn.Module):
    def __init__(self):
        super(LPIPSLoss, self).__init__()
        # LPIPS model
        self.lpips = lpips.LPIPS(net='vgg').eval()  # You can change to 'alex' or 'squeezenet'

    def forward(self, x, y):
        return self.lpips(x, y).mean()  # LPIPS returns a tensor, get the mean for a scalar loss

class CombinedLoss(nn.Module):
    def __init__(self, lambda_l1=0.8, lambda_lpips=0.2):
        super(CombinedLoss, self).__init__()
        self.l1_loss = nn.L1Loss()
        self.lpips_loss = LPIPSLoss()
        self.lambda_l1 = lambda_l1
        self.lambda_lpips = lambda_lpips

    def forward(self, pred, target):
        l1 = self.l1_loss(pred, target)
        lpips = self.lpips_loss(pred, target)
        loss = self.lambda_l1 * l1 + self.lambda_lpips * lpips
        return loss

# Example usage:
# model = CombinedLoss()
# loss = model(prediction, target)

class PerceptualVGG16Loss(nn.Module):
    def __init__(self, device='cpu'):
        super().__init__()
        #vgg = models.vgg19(pretrained=True).features[:36]  # [:36] includes relu4_4
        vgg = models.vgg16(pretrained=True).features[:9]  # relu2_2 ends at layer 9
        self.vgg = vgg.eval().to(device)
        for param in self.vgg.parameters():
            param.requires_grad = False

        self.normalize = nn.Sequential(
            nn.Conv2d(3, 3, kernel_size=1),  # Dummy layer to register mean/std buffers
        )
        mean = torch.tensor([0.485, 0.456, 0.406]).view(1,3,1,1)
        std = torch.tensor([0.229, 0.224, 0.225]).view(1,3,1,1)
        self.register_buffer('mean', mean)
        self.register_buffer('std', std)

        self.device = device

    def forward(self, x, y):
        # Normalize without modifying the original inputs
        x = x.to(self.device)
        y = y.to(self.device)

        x = (x - self.mean) / self.std
        y = (y - self.mean) / self.std


        x_features = self.vgg(x)
        y_features = self.vgg(y)

        if x_features.dim() == 0 or y_features.dim() == 0:
            raise ValueError(f"Unexpected output dimensions from VGG19: x_features: {x_features.shape}, y_features: {y_features.shape}")

        return F.l1_loss(x_features , y_features).to(device)









































"""#  RESNET18 PERCETPTUAL LOSS AND LIGHT WEIGHT VERSION OF IT"""

import torchvision.models as models
import torch.nn as nn

class ResNet18Perceptual(nn.Module):
    def __init__(self):
        super().__init__()
        resnet = models.resnet18(weights='IMAGENET1K_V1')  # or pretrained=True in older versions
        self.features = nn.Sequential(
            resnet.conv1,
            resnet.bn1,
            resnet.relu,
            resnet.maxpool,
            resnet.layer1,
            resnet.layer2,  #  we stop here
        )
        for p in self.features.parameters():
            p.requires_grad = False

    def forward(self, x):
        return self.features(x)

class CustomFeatureExtractor(nn.Module):
  def __init__(self , base_model, weights , selected_layers):
    super().__init__()

    self.base_model = getattr(models, base_model)(weights=weights)
    self.selected_layers = selected_layers
    layers = []
    for layer in selected_layers:
      layers.append(getattr(self.base_model , layer))
    self.custom = nn.Sequential(*layers)


    def forward(self , x):
      return self.custom(x)

# a dict to store the activations
activation = {}
def getActivation(name):
  # the hook signature
  def hook(model, input, output):
    activation[name] = output.detach()
  return hook

# class CustomHookLoss(nn.Module):
#   def__init__(self , base_model , weights , selected_layer) :
#     super().__init__()

#     self.selected_layer = selected_layer
#     self.base_model = getattr(models , base_models)(weights = weights).eval().to(device)
#     self.activation = {}

#     self.normalize = nn.Sequential(
#             nn.Conv2d(3 ,3  , kernel_size = 1),
#         )

#     mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)
#     std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)
#     self.rigister_buffer("mean" , mean)
#     self.rigister_buffer("std" , std)
#     self.device = device

#   def getActivation(name):
#     def hook(model , input , output):
#       self.activation[name] = output.detach()

#     return hook

#   def catch_hoo_output(self):
#     hook_out  = getattr(self.base_model , self.selected_layer).register_forward_hook(getActivation())


#   def forward(self, x , y):
#       x = x.to(self.device)
#       y = y.to(self.device)

#       x = (x - self.mean) / self.std
#       y = (y - self.mean) / self.std

#       x_features = self.base_model(x)
#       y_features = self.base_model(y)

#       return F.l1_loss(x_features , y_features).to(device)

from pathlib import Path
from PIL import Image
lena = Path("/content/Lena256.bmp")
lenaX = Image.open(lena)
lenaX

from pathlib import Path
from PIL import Image
lena_noise = Path("/content/LenaGuassian25.bmp")
lenaX_noise = Image.open(lena_noise)
lenaX_noise

import torchvision.transforms as T
transform = T.Compose([
    T.Grayscale(num_output_channels = 3),
    T.ToTensor()
])

from torchvision import transforms as T
lenatransformed = transform(lenaX)
ready_for_feed = lenatransformed.unsqueeze(0)
ready_for_feed.shape

from torchvision import transforms as T
lena_denoise = transform(lenaX_noise)
lena_denoised = lena_denoise.unsqueeze(0)
lena_denoised.shape

ready_for_feed == lena_denoised

x = F.interpolate(ready_for_feed , scale_factor= 0.5, mode='bilinear', align_corners=False)

x.shape

import time
import torch.nn.functional as F
import torchvision.models as models
start_99= time.time()
x_fee = loss(ready_for_feed)
y_fee = loss(lena_denoised)

n = F.l1_loss(y_fee , x_fee)
print(f"vgg19 loss forward time : {time.time() - start_99 } ")

n



start_11 = time.time()

out = loss_per(lena_denoised , ready_for_feed , down_factor = 0.5 , downscale = True , amp = False )
print(f"vgg19 loss forward time : {time.time() - start_11 } ")

out



import torchvision.models as models
loss = ResNet18Perceptual_two('layer1' , device)
print(f"vgg19 loss forward time : {time.time() - start } ")

import torch
import torch.nn as nn
class ResNet18Perceptual_two(nn.Module):
    def __init__(self, last_layer, device):
        super().__init__()
        self.last_layer = last_layer
        self.device = device

        self.resnet = models.resnet18(weights='IMAGENET1K_V1').eval().to(device)
        self.features = self.slice_model_until()
        for p in self.features.parameters():
            p.requires_grad = False

        mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)
        std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)
        self.register_buffer("mean", mean)
        self.register_buffer("std", std)

    # def slice_model_until(self):
    #     """ Slice a model up to and including last_layer_name """
    #     layers = []
    #     for name, layer in self.resnet.named_children():
    #         layers.append(layer)
    #         if name == self.last_layer:
    #             break
    #     return nn.Sequential(*layers)

    def slice_model_until(self):
      parts = self.last_layer.split('.')
      model = self.resnet
      layers = []

      for name, layer in model.named_children():
        layers.append(layer)
        if name == parts[0]:
            # Go deeper if necessary
          if len(parts) > 1:
            sublayer = layer
            for p in parts[1:]:
              if '[' in p:  # If it has an index like [0]
                layer_name, idx = p[:-1].split('[')
                sublayer = getattr(sublayer, layer_name)[int(idx)]
              else:
                sublayer = getattr(sublayer, p)
                layers.append(sublayer)
          break

      return nn.Sequential(*layers)

    def forward(self , x):
      x = F.interpolate(x , scale_factor= 0.5, mode='bilinear', align_corners=False)
      return self.features(x)

out = resnet(ready_for_feed)

print(activation)

# detach the hooks

import torchvision.models as models
import torch.nn as nn
import torch.nn.functional as F

class ResNet18Perceptual(nn.Module):
    def __init__(self):
        super().__init__()
        resnet = models.resnet18(weights='IMAGENET1K_V1').eval().to(device)  # or pretrained=True in older versions
        self.features = nn.Sequential(
            resnet.conv1,
            resnet.bn1,
            resnet.relu,
            resnet.maxpool,
            resnet.layer1,
            resnet.layer2,  # 👈 we stop after Layer2
        )
        for p in self.features.parameters():
            p.requires_grad = False


        self.normalize = nn.Sequential(
            nn.Conv2d(3 ,3  , kernel_size = 1),
        )

        mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)
        std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)
        self.rigister_buffer("mean" , mean)
        self.rigister_buffer("std" , std)
        self.device = device

    def forward(self, x , y):
      x = x.to(self.device)
      y = y.to(self.device)

      x = (x - self.mean) / self.std
      y = (y - self.mean) / self.std

      x_features = self.features(x)
      y_features = self.features(y)

      #if x_features.dim() == 0 or y_features.dim() == 0:
            #raise ValueError(f"Unexpected output dimensions from VGG19: x_features: {x_features.shape}, y_features: {y_features.shape}")

      return F.l1_loss(x_features , y_features).to(device)

import torchvision.models as models
import torch.nn as nn
import torch.nn.functional as F

class ResNet18Perceptual(nn.Module):
    def __init__(self, last_layer, device):
        super().__init__()
        self.last_layer = last_layer
        self.device = device

        self.resnet = models.resnet18(weights='IMAGENET1K_V1').eval().to(device)
        self.features = self.slice_model_until()
        for p in self.features.parameters():
            p.requires_grad = False

        mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)
        std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)
        self.register_buffer("mean", mean)
        self.register_buffer("std", std)

    def slice_model_until(self):
        """ Slice a model up to and including last_layer_name """
        layers = []
        for name, layer in self.resnet.named_children():
            layers.append(layer)
            if name == self.last_layer:
                break
        return nn.Sequential(*layers)

    def forward(self, x, y):

        x = x.to(self.device)
        y = y.to(self.device)

        x = (x - self.mean) / self.std
        y = (y - self.mean) / self.std

        x_features = self.features(x)
        y_features = self.features(y)

        if x_features.dim() == 0 or y_features.dim() == 0:
            raise ValueError(f"Unexpected output dimensions from ResNet18: x_features: {x_features.shape}, y_features: {y_features.shape}")

        return F.l1_loss(x_features, y_features)

from torch.amp import autocast

loss_per = ResNet18Perceptual('layer1' , device)

import torchvision.models as models
import torch.nn as nn
import torch.nn.functional as F
from torch.amp import autocast


class ResNet18Perceptual(nn.Module):
    def __init__(self, last_layer, device):
        super().__init__()
        self.last_layer = last_layer
        self.device = device

        self.resnet = models.resnet18(weights='IMAGENET1K_V1').eval().to(device)
        self.features = self.slice_model_until()
        for p in self.features.parameters():
            p.requires_grad = False

        mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)
        std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)
        self.register_buffer("mean", mean)
        self.register_buffer("std", std)

    # def slice_model_until(self):
    #     """ Slice a model up to and including last_layer_name """
    #     layers = []
    #     for name, layer in self.resnet.named_children():
    #         layers.append(layer)
    #         if name == self.last_layer:
    #             break
    #     return nn.Sequential(*layers)

    def slice_model_until(self):
      parts = self.last_layer.split('.')
      model = self.resnet
      layers = []

      for name, layer in model.named_children():
        layers.append(layer)
        if name == parts[0]:
            # Go deeper if necessary
          if len(parts) > 1:
            sublayer = layer
            for p in parts[1:]:
              if '[' in p:  # If it has an index like [0]
                layer_name, idx = p[:-1].split('[')
                sublayer = getattr(sublayer, layer_name)[int(idx)]
              else:
                sublayer = getattr(sublayer, p)
                layers.append(sublayer)
          break

      return nn.Sequential(*layers)

    # def forward(self, x, y):
    #   x = x.to(self.device)
    #   y = y.to(self.device)

    #   x = (x - self.mean) / self.std
    #   y = (y - self.mean) / self.std

    #   with autocast('cuda'):
    #     x_features = self.features(x)
    #     y_features = self.features(y)

    #     if x_features.dim() == 0 or y_features.dim() == 0:
    #         raise ValueError(f"Unexpected output dimensions: x_features: {x_features.shape}, y_features: {y_features.shape}")

    #   return F.l1_loss(x_features, y_features).to(self.device)
    def forward(self, x, y,  down_factor, downscale: bool = False , amp : bool = False ):
      x = x.to(self.device)
      y = y.to(self.device)

      x = (x - self.mean) / self.std
      y = (y - self.mean) / self.std

      return self._compute_loss(x, y, down_factor ,downscale , amp)

    def _compute_loss(self, x, y, down_factor, downscale: bool, amp : bool ):
      context = autocast() if amp else torch.cuda.amp.autocast(enabled=False)
    #with autocast(enabled = amp ) B)
      with context:
        if downscale:
            x = F.interpolate(x, scale_factor=down_factor, mode='bilinear', align_corners=False)
            y = F.interpolate(y, scale_factor=down_factor, mode='bilinear', align_corners=False)

        x_features = self.features(x)
        y_features = self.features(y)

      return F.l1_loss(x_features, y_features)

loss = ResNet18Perceptual("layer1" , s)

import torch
import torch.nn as nn
class ResNet18Perceptual_one(nn.Module):
    def __init__(self, last_layer, device):
        super().__init__()
        self.last_layer = last_layer
        self.device = device

        self.resnet = models.resnet18(weights='IMAGENET1K_V1').eval().to(device)
        self.features = self.slice_model_until()
        for p in self.features.parameters():
            p.requires_grad = False

        mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)
        std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)
        self.register_buffer("mean", mean)
        self.register_buffer("std", std)

    # def slice_model_until(self):
    #     """ Slice a model up to and including last_layer_name """
    #     layers = []
    #     for name, layer in self.resnet.named_children():
    #         layers.append(layer)
    #         if name == self.last_layer:
    #             break
    #     return nn.Sequential(*layers)

    def slice_model_until(self):
      parts = self.last_layer.split('.')
      model = self.resnet
      layers = []

      for name, layer in model.named_children():
        layers.append(layer)
        if name == parts[0]:
            # Go deeper if necessary
          if len(parts) > 1:
            sublayer = layer
            for p in parts[1:]:
              if '[' in p:  # If it has an index like [0]
                layer_name, idx = p[:-1].split('[')
                sublayer = getattr(sublayer, layer_name)[int(idx)]
              else:
                sublayer = getattr(sublayer, p)
                layers.append(sublayer)
          break

      return nn.Sequential(*layers)

    def forward(self , x):
      return self.features(x)

model = ResNet18Perceptual_one('layer1' , device)

start = time.time()
lena = model(ready_for_feed)
print(f"vgg19 loss forward time : {time.time() - start } ")

with autocast('cpu')

loss1 = ResNet18Perceptual('layer1' , device)

import torch.nn.functional as F
noise , clean = next(iter(train_dataloader))

noise = noise.to(device)
clean = clean.to(device)

with torch.no_grad():
  pred = model_denoise_2(noise)


import time
start = time.time()

loss = loss1(pred , clean , down_factor = 0.5 , downscale = True , amp = False)
print(f"vgg19 loss forward time : {time.time() - start } ")





"""# MOBILENETV2 LOSS AND IT'S LIGHT WEIGHT VERSION"""



import torchvision.models as models
import torch.nn as nn

class MobileNetV2Perceptual(nn.Module):
    def __init__(self):
        super().__init__()
        mobilenet = models.mobilenet_v2(weights='IMAGENET1K_V1')  # or pretrained=True
        self.features = mobilenet.features[:8]  # 👈 Up to the 7th bottleneck block
        for p in self.features.parameters():
            p.requires_grad = False

    def forward(self, x):
        return self.features(x)

import torch
import torch.nn as nn
import torchvision.models as models
class CustomMobileNetV2(nn.Module):
  def __init__(self , up_to_layer):
    super().__init__()
    self.up_to_layer = up_to_layer
    mobilenet = models.mobilenet_v2(weights = 'IMAGENET1K_V1')
    self.features = mobilenet.features[:self.up_to_layer]
    for p in self.features.parameters():
      p.requires_grad = False

    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)
    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)
    self.register_buffer("mean" , mean)
    self.register_buffer("std" , std)
    self.device = device

  def forward(self , x , y):
    x = x.to(device)
    y = y.to(device)

    x = (x - self.mean) / self.std
    y = (y - self.mean) / self.state_dict
    x_features =  self.features(x)
    y_features = self.features(y)

    return F.l1_loss(x_features , y_features).to(device)

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.models as models
from torch.cuda.amp import autocast

class CustomMobileNetV2(nn.Module):
    def __init__(self, up_to_layer):
        super().__init__()
        mobilenet = models.mobilenet_v2(weights='IMAGENET1K_V1')
        self.features = mobilenet.features[:up_to_layer]
        for p in self.features.parameters():
            p.requires_grad = False

        mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)
        std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)
        self.register_buffer("mean", mean)
        self.register_buffer("std", std)
        self.device = device


    def forward(self, x, y,  down_factor, downscale: bool = False, amp: bool = False ):
        x = x.to(self.device)
        y = y.to(self.device)

        x = (x - self.mean) / self.std
        y = (y - self.mean) / self.std

        return self._compute_loss(x, y, down_factor, downscale, amp)

    def _compute_loss(self, x, y, down_factor, downscale: bool, amp: bool):
        context = autocast() if amp else torch.cuda.amp.autocast(enabled=False)
        #with autocast(enabled = amp ) B)
        with context:
            if downscale:
                x = F.interpolate(x, scale_factor=down_factor, mode='bilinear', align_corners=False)
                y = F.interpolate(y, scale_factor=down_factor, mode='bilinear', align_corners=False)

            x_features = self.features(x)
            y_features = self.features(y)

            return F.l1_loss(x_features, y_features)

device = "cuda" if torch.cuda.is_available() else "cpu"
device

mobloss = CustomMobileNetV2(9)

import torch.nn.functional as F
noise , clean = next(iter(train_dataloader))

noise = noise.to(device)
clean = clean.to(device)

with torch.no_grad():
  pred = model_denoise_2(noise)


import time
start = time.time()

loss = mobloss(pred , clean , down_factor = 0.5 , downscale = True , amp = False )
print(f"vgg19 loss forward time : {time.time() - start } ")

import torchvision.models as models
import torch.nn as nn

class MobileNetV2Perceptual(nn.Module):
    def __init__(self):
        super().__init__()
        mobilenet = models.mobilenet_v2(weights='IMAGENET1K_V1')  # or pretrained=True
        self.features = mobilenet.features[:4]  # 👈 Up to the 7th bottleneck block
        for p in self.features.parameters():
            p.requires_grad = False

    def forward(self, x):
        return self.features(x)



"""# DOWNSCALE INPUTS BEFORE FEATURE EXTRACTOR"""

import torch.nn.functional as F

def forward(self, x, y):
    # Normalize x and y first

    # 🛑 Downsample by 2x
    x_small = F.interpolate(x, scale_factor=0.5, mode='bilinear', align_corners=False)
    y_small = F.interpolate(y, scale_factor=0.5, mode='bilinear', align_corners=False)

    x_features = self.backbone(x_small)
    y_features = self.backbone(y_small)

    return F.l1_loss(x_features, y_features)







"""# A CUSTOM PERCEPTUAL LOSS"""

import torch
import torch.nn as nn
import torch.nn.functional as F

class TinyPerceptualNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.backbone = nn.Sequential(
            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),

            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),  # Downsample
            nn.ReLU(inplace=True),

            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),  # Downsample
            nn.ReLU(inplace=True),

            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1), # Downsample
            nn.ReLU(inplace=True),
        )

        # Optional: Initialize weights like Kaiming Normal for stability
        for m in self.backbone.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')

        # ⚡ Freeze params
        for p in self.backbone.parameters():
            p.requires_grad = False

    def forward(self, x):
        return self.backbone(x)

class TinyPerceptualNetPlus(nn.Module):
    def __init__(self):
        super().__init__()
        self.backbone = nn.Sequential(
            nn.Conv2d(3, 16, 3, 1, 1),
            nn.ReLU(inplace=True),

            nn.Conv2d(16, 32, 3, 2, 1),  # Downsample
            nn.ReLU(inplace=True),

            nn.Conv2d(32, 64, 3, 2, 1),  # Downsample
            nn.ReLU(inplace=True),

            nn.Conv2d(64, 128, 3, 2, 1),  # Downsample
            nn.ReLU(inplace=True),

            nn.Conv2d(128, 256, 3, 2, 1),  # Final Downsample
            nn.ReLU(inplace=True),
        )

        for m in self.backbone.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')

        for p in self.backbone.parameters():
            p.requires_grad = False

    def forward(self, x):
        return self.backbone(x)

"""CONSIDER CHANGING THE LAST LAYER STRIDE TO 1 AND THEN EXPERIMENT ."""

# ADAPTIVE AVEREAGE POOL :
self.avgpool = nn.AdaptiveAvgPool2d((4, 4))  # or (1,1) if you want super compact features

#FOR A FIX SIZE OUTPUT :
def forward(self, x):
    x = self.backbone(x)
    x = self.avgpool(x)
    return x



"""# DYNAMIC FEATURE EXTRACTOR"""

import torch
import torch.nn as nn

class DynamicFeatureExtractor(nn.Module):
    def __init__(self,
                 input_channels=3,
                 layer_channels=[16, 32, 64, 128, 256],
                 strides=None,
                 kernel_sizes=None,
                 use_bn=True,
                 activation="relu",
                 final_avgpool=True,
                 pool_size=(4, 4)
                 ):
        super().__init__()

        layers = []

        if strides is None:
            strides = [2] * len(layer_channels)  # Default: downsample every block
        if kernel_sizes is None:
            kernel_sizes = [3] * len(layer_channels)

        assert len(layer_channels) == len(strides) == len(kernel_sizes), "lengths must match!"

        in_channels = input_channels

        act_fn = {
            "relu": nn.ReLU(inplace=True),
            "leaky_relu": nn.LeakyReLU(0.2, inplace=True),
            "gelu": nn.GELU()
        }[activation.lower()]

        for out_channels, stride, ksize in zip(layer_channels, strides, kernel_sizes):
            padding = (ksize - 1) // 2
            layers.append(nn.Conv2d(in_channels, out_channels, ksize, stride=stride, padding=padding))

            if use_bn:
                layers.append(nn.BatchNorm2d(out_channels))

            layers.append(act_fn)

            in_channels = out_channels  # update

        self.backbone = nn.Sequential(*layers)

        self.final_avgpool = nn.AdaptiveAvgPool2d(pool_size) if final_avgpool else nn.Identity()

    def forward(self, x):
        x = self.backbone(x)
        x = self.final_avgpool(x)
        return x

a = [2] * 3
a

newloss = LossWithPersonalFE(3 , [16, 32, 64, 128, 256] , [1, 2, 2, 2, 1], [3,3,3,3,3] , batchnorm = True , activation = 'relu' , fin_avgpool = True ,pool_size = (4,4)  )

import torch.nn.functional as F
noise , clean = next(iter(train_dataloader))

noise = noise.to(device)
clean = clean.to(device)

with torch.no_grad():
  pred = model_denoise_2(noise)


import time
start = time.time()

loss = newloss(pred , clean , down_factor = 0.5 , downscale = True  , amp = False  )
print(f"vgg19 loss forward time : {time.time() - start } ")

net = DynamicFeatureExtractor(
    input_channels=3,
    layer_channels=[16, 32, 64, 128, 256],
    strides=[1, 2, 1, 2, 1],  # Control where you downsample
    kernel_sizes=[3, 3, 5, 3, 3],  # Mix 3x3 and 5x5
    use_bn=True,
    activation="gelu",
    final_avgpool=True,
    pool_size=(4,4)
)

print(net)

class LossWithPersonalFE(nn.Module):
  def __init__(self, In : int, layer : list, stride : list, kernel : list,
               batchnorm : bool, activation : str, fin_avgpool : bool,
               pool_size : tuple ):
    super().__init__()
    self.input = In
    self.layers = layer
    self.stride = stride
    self.kernel = kernel
    self.use_bn = batchnorm
    self.activation = activation
    self.final_avgpool = fin_avgpool
    self.pool_size = pool_size
    self.custom_FE = DynamicFeatureExtractor(self.input, self.layers, self.stride,
                                             self.kernel, self.use_bn, self.activation,
                                             self.final_avgpool, self.pool_size)


    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)
    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)
    self.register_buffer("mean", mean)
    self.register_buffer("std", std)
    self.device = device


  def forward(self, x, y,  down_factor, downscale: bool = False , amp : bool = False ):
    x = x.to(self.device)
    y = y.to(self.device)

    x = (x - self.mean) / self.std
    y = (y - self.mean) / self.std

    return self._compute_loss(x, y, down_factor ,downscale , amp)

  def _compute_loss(self, x, y, down_factor, downscale: bool, amp : bool ):
    context = autocast() if amp else torch.cuda.amp.autocast(enabled=False)
    #with autocast(enabled = amp ) B)
    with context:
      if downscale:
          x = F.interpolate(x, scale_factor=down_factor, mode='bilinear', align_corners=False)
          y = F.interpolate(y, scale_factor=down_factor, mode='bilinear', align_corners=False)

      x_features = self.custom_FE(x)
      y_features = self.custom_FE(y)

      return F.l1_loss(x_features, y_features)



"""# DYNAMIC SELECTING LAYERS FROM THE MODEL , WITH SLICING AND HOOKS (●'◡'●)"""

submodel = nn.Sequential(*list(full_model.children())[:5])

# HOOKS


features = {}

def hook_fn(module, input, output):
    features[id(module)] = output

# Attach hook to selected layers
selected_layers = [layer1, layer2, layer3]  # (whatever layers you want)
for layer in selected_layers:
    layer.register_forward_hook(hook_fn)

out = model(x)
# Now `features` dict has all selected layer outputs!!



import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.models as models

class CustomHookLoss(nn.Module):
    def __init__(self, base_model, weights, selected_layer, device):
        super().__init__()

        self.device = device
        self.selected_layer = selected_layer
        self.base_model = getattr(models, base_model)(weights=weights).eval().to(device)
        self.activation = {}

        mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)
        std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)
        self.register_buffer("mean", mean)
        self.register_buffer("std", std)

        self.normalize = nn.Sequential(
            nn.Conv2d(3, 3, kernel_size=1)
        )

        # Setup hook
        self.catch_hook_output()

    def getActivation(self, name):
        def hook(model, input, output):
            self.activation[name] = output.detach()
        return hook

    def catch_hook_output(self):
        layer = self._get_nested_layer(self.base_model, self.selected_layer)
        layer.register_forward_hook(self.getActivation(self.selected_layer))

    def _get_nested_layer(self, model, layer_path):
        """Helper to get nested layers like layer3[0].downsample[1]"""
        names = layer_path.replace(']', '').split('[')
        for name in names:
            if name.isdigit():
                model = model[int(name)]
            else:
                model = getattr(model, name)
        return model

    def forward(self, x, y):
        x = x.to(self.device)
        y = y.to(self.device)

        x = (x - self.mean) / self.std
        y = (y - self.mean) / self.std

        self.activation = {}
        self.base_model(x)
        x_features = self.activation[self.selected_layer]

        self.activation = {}
        self.base_model(y)
        y_features = self.activation[self.selected_layer]

        return F.l1_loss(x_features, y_features).to(self.device)

"""# A Custom Class for initializing new base models for using in perceptual losses"""



"""It also have mixed precision on cuda"""

import torch
import torch.nn as nn
import torchvision.models as models
from torch.cuda.amp import autocast

class CustomFeatureExtractor(nn.Module):
    def __init__(self, model_name, weights, selected_layers):
        super().__init__()

        # Initialize the base model
        self.base_model = getattr(models, model_name)(weights=getattr(models, weights))
        self.selected_layers = selected_layers
        self.selected_modules = nn.ModuleList(self._get_layers())

    def _get_layers(self):
        layers = []
        for layer_name in self.selected_layers:
            module = self.get_module_by_path(self.base_model, layer_name)
            layers.append(module)
        return layers

    def get_module_by_path(self, model, path):
        current_module = model
        tokens = path.replace(']', '').split('[')
        final_tokens = []
        for token in tokens:
            parts = token.split('.')
            final_tokens.extend(parts)

        for token in final_tokens:
            if token.isdigit():
                current_module = current_module[int(token)]
            else:
                current_module = getattr(current_module, token)
        return current_module

    def forward(self, x):
        outputs = []
        with autocast(device_type='cuda', dtype=torch.float16):
            for module in self.selected_modules:
                x = module(x)
                outputs.append(x)
        return outputs



"""# Visualize feature maps"""

import matplotlib.pyplot as plt

# Suppose `activation['layer1']` contains your hook output
features = activation['layer1'].squeeze(0)  # remove batch dim --> shape [64, 64, 64]

# Plot first 8 feature maps
num_features = 8
fig, axes = plt.subplots(1, num_features, figsize=(20, 5))
for idx in range(num_features):
    axes[idx].imshow(features[idx].cpu().detach().numpy(), cmap='viridis')
    axes[idx].axis('off')
plt.show()

img = (img - img.min()) / (img.max() - img.min())  # normalize between 0 and 1
axes[idx].imshow(img, cmap='viridis')

img = features[idx].cpu().detach().numpy()
img = (img - img.min()) / (img.max() - img.min())  # normalize between 0 and 1
axes[idx].imshow(img, cmap='viridis')









# HOW TO USE IT


feature_extractor = TinyPerceptualNet().to(device)

# During loss computation
features_pred = feature_extractor(predicted_images)
features_gt = feature_extractor(ground_truth_images)

loss = F.l1_loss(features_pred, features_gt)

# WE CAN ADD BATCHNORM AFTER EVERY CONV , AND BEFORE RELU FOR A MORE VGG-ISH STYLE ,
# LIKE THIS
nn.Conv2d(3, 16, 3, 1, 1),
nn.BatchNorm2d(16),
nn.ReLU(inplace=True),

mean = [0.485, 0.456, 0.406]
std  = [0.229, 0.224, 0.225]

#FOR MEAN AND STD



"""# Optimizer and metrices"""

from torch.optim import Adam
optimizer = torch.optim.Adam(model_denoise_2.parameters(), lr=1e-3, betas=(0.9, 0.999), eps=1e-08)

scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5, threshold = 0.01)

!pip install torchmetrics

from torchmetrics.functional import structural_similarity_index_measure as ssim
from torchmetrics.functional import peak_signal_noise_ratio
#import lpips
#loss_fn = nn.BCELoss()

#l1_loss = nn.L1Loss()
#VGG_Loss = VGGPerceptualLoss()

#def Total_Loss(noisy , clean):
#  l1_loss_value = l1_loss(noisy , clean)
#  VGG_Loss_value = VGG_Loss(noisy , clean)
#  return 1.0 * l1_loss_value + 0.1 * VGG_Loss_value
#lpips_fn = lpips.LPIPS(net='vgg')  # Can also be 'alex' or 'squeeze'

# Move to GPU if needed
#lpips_fn.cuda()
#def Total_Loss(pred, target, alpha=1.0, beta=0.1):
 #   return alpha * l1_loss(pred, target) + beta * VGG_Loss(normalize_resize_vgg(pred , resize = True ), normalize_resize_vgg(target, resize = True), feature_layers = [1,2])
import torch
import torch.nn as nn

# class TotalLoss(nn.Module):
#     def __init__(self, alpha=1.0, beta=0.1):
#         super(TotalLoss, self).__init__()
#         self.alpha = alpha
#         self.beta = beta
#         self.l1_loss = nn.L1Loss()
#         self.lpips = lpips_fn  # Make sure this is already defined

#     def forward(self, pred, target):
#         #pred_norm = normalize_resize_vgg(pred, resize=True)
#         #target_norm = normalize_resize_vgg(target, resize=True)
#         l1 = self.l1_loss(pred, target)
#         perceptual = self.lpips(pred , target)

#         return self.alpha * l1 + self.beta * perceptual




#loss_fn = nn.L1Loss()

#optimizer = torch.optim.Adam(model_denoise_2.parameters(),
 #                           lr= 1e-3 )

PSNR_11 = peak_signal_noise_ratio
ssim_11 = ssim

"""# new train and test loop"""

from tqdm.auto import tqdm
import torchmetrics


def train_step(model: torch.nn.Module,
               data_loader: torch.utils.data.DataLoader,
               loss_fn: torch.nn.Module,
               optimizer: torch.optim.Optimizer,
               PSNR : torchmetrics.functional,
               ssim : torchmetrics.functional,
               device: torch.device = device):
    train_loss, train_psnr , ssim_train  = 0, 0, 0
    model.train()
    model.to(device)
    for batch, (noise, clean , clean_feat , noise_info) in enumerate(data_loader):
        # Send data to GPU
        noise, clean , clean_feat , noise_info = noise.to(device), clean.to(device) , clean_feat.to(device) , noise_info.to(device)

        # 1. Forward pass
        pred = model(noise)

        # 2. Calculate loss
        ##############################loss = loss_fn(pred, clean)
        #train_loss += loss
        loss , loss_dict = loss_fn(pred , clean_feat)
        train_loss += loss.item()
        #train_acc += accuracy_fn(y_true=y,
        #                         y_pred=y_pred.argmax(dim=1)) # Go from logits -> pred labels

        psnr_batch = PSNR(pred, clean, data_range=1.0)
        train_psnr += psnr_batch.item()
        #
        # loss = Total_Loss(pred , clean)
        # train_loss += loss.item()

        ssim_batch = ssim(pred , clean, data_range =1.0 )
        ssim_train += ssim_batch.item()
        # 3. Optimizer zero grad
        optimizer.zero_grad()

        # 4. Loss backward
        loss.backward()

        # 5. Optimizer step
        optimizer.step()

    # Calculate loss and accuracy per epoch and print out what's happening
    train_loss /= len(data_loader)
    train_psnr /= len(data_loader)
    ssim_train /= len(data_loader)
    print(f"Train loss: {train_loss:.5f} | Train PSNR: {train_psnr:.5f}% | Train SSIM: {ssim_train}")


def test_step(data_loader: torch.utils.data.DataLoader,
              model: torch.nn.Module,
              loss_fn: torch.nn.Module,
              PSNR : torchmetrics.functional,
              ssim : torchmetrics.functional,
              device: torch.device = device ):
    test_loss, test_psnr , ssim_test = 0, 0, 0
    model.to(device)
    model.eval() # put model in eval mode
    # Turn on inference context manager

    #TODO : torch.zero_grad ??

    with torch.inference_mode():
        for noise_test, clean_test in data_loader:
            # Send data to GPU
            noise_test , clean_test = noise_test.to(device), clean_test.to(device)

            # 1. Forward pass
            test_pred = model(noise_test)

            # 2. Calculate loss and accuracy
            #test_loss += loss_fn(test_pred, clean_test)
            #################################test_loss += loss_fn(test_pred , clean_test).item()
            loss , loss_dict = loss_fn(test_pred , clean_test)
            test_loss += loss.item()
            #test_acc += accuracy_fn(y_true=y,
            #    y_pred=test_pred.argmax(dim=1) # Go from logits -> pred labels
            #)
            psnr_batch = PSNR(test_pred , clean_test , data_range = 1.0)
            test_psnr += psnr_batch.item()

            ssim_batch = ssim(test_pred , clean_test)
            ssim_test += ssim_batch.item()

        # Adjust metrics and print out
        test_loss /= len(data_loader)
        test_psnr /= len(data_loader)
        ssim_test /= len(data_loader)
        print(f"Test loss: {test_loss:.5f} | Test PSNR: {test_psnr:.5f}%\n | Test SSIM:{ssim_test:.5f}\n")
        return test_loss , test_psnr , ssim_test

import torch
 torch.cuda.empty_cache()

from torch.cuda.amp import autocast

def train_step(data_loader, model, loss_fn, optimizer, PSNR, ssim, scaler):
    model.train()
    running_loss = 0.0
    running_psnr = 0.0
    running_ssim = 0.0

    for batch in data_loader:
        noisy_imgs, clean_imgs = batch
        noisy_imgs = noisy_imgs.to(model.device)
        clean_imgs = clean_imgs.to(model.device)

        optimizer.zero_grad()

        with autocast():
            outputs = model(noisy_imgs)
            loss = loss_fn(outputs, clean_imgs)

        # Mixed precision backward + step
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        if epoch % 5 == 0 and batch_idx == 0:  # or use `if global_step % 50 == 0`
            noise_info_sample = info_batch[0]  # info_batch is a list/dict per image
            logger.report_text(
                f"Epoch {epoch} - Sample Noise: {noise_info_sample}",
                iteration=epoch * len(train_loader) + batch_idx
            )

        running_loss += loss.item()
        running_psnr += PSNR(outputs, clean_imgs).item()
        running_ssim += ssim(outputs, clean_imgs).item()

    # Optionally return epoch stats
    return running_loss / len(data_loader), running_psnr / len(data_loader), running_ssim / len(data_loader)

from torchvision.transforms import Resize, ToPILImage
from clearml import Logger
from PIL import Image
import io



def test_step(data_loader: torch.utils.data.DataLoader,
              model: torch.nn.Module,
              loss_fn: torch.nn.Module,
              PSNR: torchmetrics.functional,
              ssim: torchmetrics.functional,
              device: torch.device = device,
              epoch: int = 0,  # Pass current epoch here
              image_log_interval: int = 5):

    test_loss, test_psnr, ssim_test = 0, 0, 0
    model.to(device)
    model.eval()

    log_images = (epoch % image_log_interval == 0)
    logged_images = 0  # Limit how many images we log
    resize = Resize((128, 128))  # Resize images to 128×128
    to_pil = ToPILImage()

    with torch.inference_mode():
        for noise_test, clean_test , clean_feet , noise_info in data_loader:
            noise_test, clean_test , clean_feat , noise_info = noise_test.to(device), clean_test.to(device) , clean_feat.to(device) , noise_info.to(devic)
            test_pred = model(noise_test)

            # Loss and metrics
            loss, _ = loss_fn(test_pred, clean_feat)
            test_loss += loss.item()
            psnr_batch = PSNR(test_pred, clean_test, data_range=1.0)
            ssim_batch = ssim(test_pred, clean_test, data_range=1.0)
            test_psnr += psnr_batch.item()
            ssim_test += ssim_batch.item()

            # Only log a few samples from the first batch
            if log_images and logged_images == 0:
                logger = Logger.current_logger()

                for i in range(min(4, noise_test.size(0))):  # Log first 4 samples
                    # Resize and convert to PIL
                    input_img = to_pil(resize(noise_test[i].cpu()))
                    output_img = to_pil(resize(test_pred[i].cpu()))
                    target_img = to_pil(resize(clean_test[i].cpu()))

                    # Log to ClearML
                    logger.report_image(
                        title="Noisy Input",
                        series=f"Sample {i}",
                        iteration=epoch,
                        image=input_img
                    )
                    logger.report_image(
                        title="Denoised Output",
                        series=f"Sample {i}",
                        iteration=epoch,
                        image=output_img
                    )
                    logger.report_image(
                        title="Ground Truth",
                        series=f"Sample {i}",
                        iteration=epoch,
                        image=target_img
                    )

                logged_images = 1  # Prevent logging more than once

    test_loss /= len(data_loader)
    test_psnr /= len(data_loader)
    ssim_test /= len(data_loader)

    print(f"Test loss: {test_loss:.5f} | Test PSNR: {test_psnr:.5f}% | Test SSIM: {ssim_test:.5f}")

    return test_loss, test_psnr, ssim_test





from torchvision.transforms import Resize, ToPILImage
from clearml import Logger
from PIL import Image
import io

def test_step(data_loader: torch.utils.data.DataLoader,
              model: torch.nn.Module,
              loss_fn: torch.nn.Module,
              PSNR: torchmetrics.functional,
              ssim: torchmetrics.functional,
              device: torch.device = device,
              epoch: int = 0,  # Pass current epoch here
              image_log_interval: int = 5):

    test_loss, test_psnr, ssim_test = 0, 0, 0
    model.to(device)
    model.eval()

    log_images = (epoch % image_log_interval == 0)
    logged_images = 0  # Limit how many images we log
    resize = Resize((128, 128))  # Resize images to 128×128
    to_pil = ToPILImage()

    logger = Logger.current_logger()

    with torch.inference_mode():
        for batch_idx, batch in enumerate(data_loader):
            # Unpack batch depending on whether noise info is included
            if len(batch) == 2:
                noise_test, clean_test = batch
                info_batch = [{} for _ in range(len(noise_test))]
            else:
                noise_test, clean_test, info_batch = batch

            noise_test, clean_test = noise_test.to(device), clean_test.to(device)
            test_pred = model(noise_test)

            # Loss and metrics
            loss, _ = loss_fn(test_pred, clean_test)
            test_loss += loss.item()
            psnr_batch = PSNR(test_pred, clean_test, data_range=1.0)
            ssim_batch = ssim(test_pred, clean_test, data_range=1.0)
            test_psnr += psnr_batch.item()
            ssim_test += ssim_batch.item()

            # Only log a few samples from the first batch
            if log_images and logged_images == 0:
                for i in range(min(4, noise_test.size(0))):  # Log first 4 samples
                    # Resize and convert to PIL
                    input_img = to_pil(resize(noise_test[i].cpu()))
                    output_img = to_pil(resize(test_pred[i].cpu()))
                    target_img = to_pil(resize(clean_test[i].cpu()))

                    # Log images to ClearML
                    logger.report_image(
                        title="Noisy Input",
                        series=f"Sample {i}",
                        iteration=epoch,
                        image=input_img
                    )
                    logger.report_image(
                        title="Denoised Output",
                        series=f"Sample {i}",
                        iteration=epoch,
                        image=output_img
                    )
                    logger.report_image(
                        title="Ground Truth",
                        series=f"Sample {i}",
                        iteration=epoch,
                        image=target_img
                    )

                    # Log noise metadata if available
                    info = info_batch[i]
                    if info:
                        logger.report_text(
                            f"Epoch {epoch} - Sample {i} Noise Info: {info}",
                            iteration=epoch
                        )

                logged_images = 1  # Prevent logging more than once

    test_loss /= len(data_loader)
    test_psnr /= len(data_loader)
    ssim_test /= len(data_loader)

    print(f"Test loss: {test_loss:.5f} | Test PSNR: {test_psnr:.5f}% | Test SSIM: {ssim_test:.5f}")

    return test_loss, test_psnr, ssim_test



test_loss, test_psnr, ssim_test = test_step(
    data_loader=test_dataloader,
    model=model_denoise_2,
    loss_fn=loss_fn,
    PSNR=PSNR_11,
    ssim=ssim_11,
    epoch=epoch  # NEW: for conditional logging
)

from clearml import Task
from datetime import datetime


# ==== 🔧 Experiment Configuration ====
experiment_config = {
    'model': 'UNet',
    'loss_type': 'L1 + Perceptual',
    'dataset': 'CelebA',
    'version': 'v4',
    'batch_size': 32,
    'epochs': 400,
    'lr': 3e-4,
    'use_mixed_precision': True,
    'optimizer': 'Adam',
    'scheduler': 'ReduceLROnPlateau',
    'image_log_interval': 5,
    'gradient_log_interval': 10
}

#experiment_config['version'] = datetime.now().strftime("%Y-%m-%d_%H-%M")
experiment_config['version'] = "v" + datetime.now().strftime("%Y%m%d_%H%M")


# ==== 🏷️ Dynamic Name + Tags ====
task_name = f"{experiment_config['model']}_{experiment_config['loss_type']}_{experiment_config['dataset']}_{experiment_config['version']}"
tags = [
    experiment_config['model'],
    experiment_config['loss_type'],
    experiment_config['dataset'],
    experiment_config['version']
]
if experiment_config['use_mixed_precision']:
    tags.append("mixed_precision")

# ==== 🚀 Init ClearML Task ====
task = Task.init(
    project_name="DenoisingExperiments",
    task_name=task_name,
    tags=tags
)

# ==== 📊 Log Hyperparameters ====
task.connect(experiment_config)

from torch.cuda.amp import GradScaler
from timeit import default_timer as timer
from clearml import Logger

# Settings
torch.manual_seed(42)
epochs = 400
gradient_log_interval = 10
image_log_interval = 5
track_layers = ['encoder.conv1', 'decoder.final_conv']
logger = Logger.current_logger()
scaler = GradScaler()

# Timing
train_time_start_on_gpu = timer()

for epoch in range(epochs):
    print(f"\nEpoch: {epoch}\n" + "-" * 20)

    train_dataset.set_epoch(epoch , total_epochs = 100 )

    # === Train ===
    train_loss, train_psnr, train_ssim = train_step(
        # TODO : change this
        #data_loader=train_dataloader,
        model=model_denoise_2,
        loss_fn=loss_fn,
        optimizer=optimizer,
        PSNR=PSNR_11,
        ssim=ssim_11,
        scaler=scaler
    )

    # === Test ===
    test_loss, test_psnr, test_ssim = test_step(
        data_loader=test_dataloader,
        model=model_denoise_2,
        loss_fn=loss_fn,
        PSNR=PSNR_11,
        ssim=ssim_11,
        epoch=epoch,
        image_log_interval=image_log_interval
    )

    # === Log Gradients ===
    if epoch % gradient_log_interval == 0:
        for name, param in model_denoise_2.named_parameters():
            if any(layer in name for layer in track_layers):
                if param.requires_grad and param.grad is not None:
                    logger.report_scalar(
                        title="Gradients",
                        series=name,
                        value=param.grad.norm().item(),
                        iteration=epoch
                    )


    logger.report_scalar("Loss", "train", train_loss, iteration=epoch)
    logger.report_scalar("PSNR", "train", train_psnr, iteration=epoch)
    logger.report_scalar("SSIM", "train", train_ssim, iteration=epoch)



    logger.report_scalar("Loss", "Test", test_loss, iteration=epoch)
    logger.report_scalar("PSNR", "Test", test_psnr, iteration=epoch)
    logger.report_scalar("SSIM", "Test", test_ssim, iteration=epoch)

    # === Adjust LR ===
    scheduler.step(test_ssim)
    print(f"Learning rate: {scheduler.get_last_lr()[0]:.6f}")

    # === Save Checkpoint ===
    if (epoch + 1) % 5 == 0:
        save_checkpoint(model_denoise_2, optimizer, epoch, test_loss, test_psnr, test_ssim,
                        filename=f"checkpoint_epoch_{epoch + 1}.pth")

# === Final Timing ===
train_time_end_on_gpu = timer()
total_train_time_model_1 = print_train_time(
    start=train_time_start_on_gpu,
    end=train_time_end_on_gpu,
    device=device
)

from timeit import default_timer as timer
from torch.cuda.amp import autocast, GradScaler
from clearml import Logger

scaler = GradScaler()
logger = Logger.current_logger()

torch.manual_seed(42)
train_time_start_on_gpu = timer()

epochs = 400
gradient_log_interval = 10  # Log gradients every 10 epochs
track_layers = ['encoder.conv1', 'decoder.final_conv']
log_images = True
logged_images = 3  # Number of samples to log

for epoch in tqdm(range(epochs)):
    print(f"Epoch: {epoch}\n---------")
    model_denoise_2.train()
    train_loss, train_psnr, train_ssim = 0, 0, 0

    for batch_idx, (noise, clean) in enumerate(train_dataloader):
        noise, clean = noise.to(device), clean.to(device)

        optimizer.zero_grad()
        with autocast():
            pred = model_denoise_2(noise)
            loss, _ = loss_fn(pred, clean)

        scaler.scale(loss).backward()

        # Log gradients for selected layers
        if epoch % gradient_log_interval == 0:
            for name, param in model_denoise_2.named_parameters():
                if any(layer in name for layer in track_layers):
                    if param.requires_grad and param.grad is not None:
                        logger.report_scalar(
                            title="Gradients",
                            series=name,
                            value=param.grad.norm().item(),
                            iteration=epoch
                        )

        scaler.step(optimizer)
        scaler.update()

        with torch.no_grad():
            train_loss += loss.item()
            train_psnr += PSNR_11(pred, clean, data_range=1.0).item()
            train_ssim += ssim_11(pred, clean, data_range=1.0).item()

    # Evaluate
    model_denoise_2.eval()
    test_loss, test_psnr, test_ssim = 0, 0, 0
    with torch.inference_mode(), autocast():
        for i, (noise, clean) in enumerate(test_dataloader):
            noise, clean = noise.to(device), clean.to(device)
            pred = model_denoise_2(noise)
            loss, _ = loss_fn(pred, clean)

            test_loss += loss.item()
            test_psnr += PSNR_11(pred, clean, data_range=1.0).item()
            test_ssim += ssim_11(pred, clean, data_range=1.0).item()

            # Log example images (first few samples of first batch only)
            if log_images and epoch % 10 == 0 and i == 0:
                for j in range(min(logged_images, noise.size(0))):
                    input_img = noise[j].detach().cpu().permute(1, 2, 0).numpy()
                    target_img = clean[j].detach().cpu().permute(1, 2, 0).numpy()
                    output_img = pred[j].detach().cpu().permute(1, 2, 0).numpy()
                    logger.report_image("Sample", f"Input_epoch{epoch}_img{j}", iteration=epoch, image=input_img)
                    logger.report_image("Sample", f"Output_epoch{epoch}_img{j}", iteration=epoch, image=output_img)
                    logger.report_image("Sample", f"Target_epoch{epoch}_img{j}", iteration=epoch, image=target_img)

    # Log metrics
    n_train = len(train_dataloader)
    n_test = len(test_dataloader)

    logger.report_scalar("Loss", "Train", iteration=epoch, value=train_loss / n_train)
    logger.report_scalar("Loss", "Test", iteration=epoch, value=test_loss / n_test)
    logger.report_scalar("PSNR", "Train", iteration=epoch, value=train_psnr / n_train)
    logger.report_scalar("PSNR", "Test", iteration=epoch, value=test_psnr / n_test)
    logger.report_scalar("SSIM", "Train", iteration=epoch, value=train_ssim / n_train)
    logger.report_scalar("SSIM", "Test", iteration=epoch, value=test_ssim / n_test)

    scheduler.step(test_ssim / n_test)
    print(f"Learning rate: {scheduler.get_last_lr()[0]}")

    if (epoch + 1) % 5 == 0:
        save_checkpoint(model_denoise_2, optimizer, epoch, test_loss, test_psnr, test_ssim,
                        filename=f"checkpoint_epoch_{epoch + 1}.pth")

train_time_end_on_gpu = timer()
total_train_time_model_1 = print_train_time(start=train_time_start_on_gpu,
                                            end=train_time_end_on_gpu,
                                            device=device)

# the origianl test function
from torchvision.transforms import Resize, ToPILImage
from clearml import Logger
from PIL import Image

def test_step(data_loader: torch.utils.data.DataLoader,
              model: torch.nn.Module,
              loss_fn: torch.nn.Module,
              PSNR: torchmetrics.functional,
              ssim: torchmetrics.functional,
              device: torch.device = device,
              epoch: int = 0,
              image_log_interval: int = 5):

    test_loss, test_psnr, ssim_test = 0, 0, 0
    model.to(device)
    model.eval()

    logger = Logger.current_logger()
    log_images = (epoch % image_log_interval == 0)
    logged_images = 0
    resize = Resize((128, 128))
    to_pil = ToPILImage()

    with torch.inference_mode():
        for noise_test, clean_test, clean_feat, noise_info in data_loader:
            noise_test = noise_test.to(device)
            clean_test = clean_test.to(device)
            clean_feat = clean_feat.to(device)
            noise_info = noise_info.to(device)

            test_pred = model(noise_test)

            loss, _ = loss_fn(test_pred, clean_feat)
            test_loss += loss.item()

            psnr_batch = PSNR(test_pred, clean_test, data_range=1.0)
            ssim_batch = ssim(test_pred, clean_test, data_range=1.0)
            test_psnr += psnr_batch.item()
            ssim_test += ssim_batch.item()

            if log_images and logged_images == 0:
                for i in range(min(4, noise_test.size(0))):
                    input_img = to_pil(resize(noise_test[i].cpu()))
                    output_img = to_pil(resize(test_pred[i].cpu()))
                    target_img = to_pil(resize(clean_test[i].cpu()))

                    logger.report_image("Noisy Input", f"Sample {i}", iteration=epoch, image=input_img)
                    logger.report_image("Denoised Output", f"Sample {i}", iteration=epoch, image=output_img)
                    logger.report_image("Ground Truth", f"Sample {i}", iteration=epoch, image=target_img)

                    # Log noise_info (can be scalar or tensor depending on your use)
                    if noise_info.ndim == 1 or noise_info.ndim == 2:
                        logger.report_scalar(
                            title="Noise Info",
                            series=f"Sample {i}",
                            value=noise_info[i].item() if noise_info.ndim == 1 else noise_info[i].mean().item(),
                            iteration=epoch
                        )
                    else:
                        logger.report_text(f"Noise Info Sample {i} @ Epoch {epoch}", str(noise_info[i].tolist()))

                logged_images = 1

    test_loss /= len(data_loader)
    test_psnr /= len(data_loader)
    ssim_test /= len(data_loader)

    print(f"Test loss: {test_loss:.5f} | Test PSNR: {test_psnr:.5f}% | Test SSIM: {ssim_test:.5f}")

    # Log scalar metrics
    logger.report_scalar("Loss", "Test", test_loss, iteration=epoch)
    logger.report_scalar("PSNR", "Test", test_psnr, iteration=epoch)
    logger.report_scalar("SSIM", "Test", ssim_test, iteration=epoch)

    return test_loss, test_psnr, ssim_test







from timeit import default_timer as timer
def print_train_time(start: float, end: float, device: torch.device = None):
    """Prints difference between start and end time.

    Args:
        start (float): Start time of computation (preferred in timeit format).
        end (float): End time of computation.
        device ([type], optional): Device that compute is running on. Defaults to None.

    Returns:
        float: time between start and end in seconds (higher is longer).
    """
    total_time = end - start
    print(f"Train time on {device}: {total_time:.3f} seconds")
    return total_time

def save_checkpoint(model, optimizer, epoch, loss, psnr, ssim , filename="checkpoint.pth"):
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': loss,
        'psnr': psnr,
        'ssim' : ssim
    }
    torch.save(checkpoint, filename)
    print(f"Checkpoint saved at {filename}")

def load_checkpoint(model, optimizer, filename="checkpoint.pth" , device="cpu"):
    checkpoint = torch.load(filename, map_location=device)

    # Load the model state dict
    model.load_state_dict(checkpoint['model_state_dict'])

    # Load the optimizer state dict
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

    epoch = checkpoint['epoch']
    loss = checkpoint['loss']
    psnr = checkpoint['psnr']
    ssim = checkpoint['ssim']

    print(f"Checkpoint loaded from {filename}")
    print(f"Resuming from epoch {epoch}, loss {loss:.5f}, PSNR {psnr:.2f} , SSIM {ssim:.5f}")

    return epoch, loss, psnr , ssim

torch.manual_seed(42)

# Measure time
from timeit import default_timer as timer
train_time_start_on_gpu = timer()

epochs = 400
for epoch in tqdm(range(epochs)):
    print(f"Epoch: {epoch}\n---------")
    train_step(data_loader=train_dataloader,
        model= model_denoise_2,
        loss_fn=loss_fn,
        optimizer=optimizer,
        PSNR=PSNR_11,
        ssim = ssim_11
    )
    test_loss , test_psnr , ssim_test = test_step(data_loader=test_dataloader,
        model= model_denoise_2,
        loss_fn=loss_fn,
        PSNR = PSNR_11,
        ssim = ssim_11
    )

    scheduler.step(ssim_test)
    print(f"Learning rate: {scheduler.get_last_lr()[0]}")

    if (epoch + 1) % 5 == 0:  # Save every 5 epochs, or adjust as needed
        save_checkpoint(model_denoise_2, optimizer, epoch, test_loss, test_psnr, ssim_test , filename=f"checkpoint_epoch_{epoch + 1}.pth")
train_time_end_on_gpu = timer()
total_train_time_model_1 = print_train_time(start=train_time_start_on_gpu,
                                            end=train_time_end_on_gpu,
                                            device=device)



"""#Curriculum learning trying different methods"""



if epoch < 10:
    noise_levels = [0.3, 0.4]
elif epoch < 20:
    noise_levels = [0.2, 0.3]
else:
    noise_levels = [0.05, 0.1, 0.2]

do you have any other idea , regardless of what these authors did in their papers ?



noise_mixin = NoisyImageMixin()



noise_mixin.set_epoch(current_epoch, total_epochs=100)

import random
import torch

class NoisyImageMixin:
    def __init__(self):
        self.current_epoch = 0
        self.total_epochs = 100  # You can overwrite this later if needed

    def set_epoch(self, epoch, total_epochs=None):
        self.current_epoch = epoch
        if total_epochs is not None:
            self.total_epochs = total_epochs

    def get_intensity_scale(self):
        # Scale goes from 1.0 (max noise) to 0.2 (light noise) as epochs progress
        progress = self.current_epoch / self.total_epochs
        intensity_scale = 1.0 - 0.8 * progress
        return max(intensity_scale, 0.2)

    def add_gaussian_noise(self, img):
        scale = self.get_intensity_scale()
        noise_factor = random.uniform(0.05 * scale, 0.5 * scale)
        noise = noise_factor * torch.randn_like(img)
        noisy = torch.clamp(img + noise, 0., 1.)
        return noisy, {'type': 'gaussian', 'factor': noise_factor}

    def add_salt_and_pepper_noise(self, img):
        scale = self.get_intensity_scale()
        amount = random.uniform(0.01 * scale, 0.1 * scale)
        s_vs_p = random.uniform(0.3, 0.7)

        noisy = img.clone()
        c, h, w = noisy.shape
        num_salt = int(amount * h * w * s_vs_p)
        num_pepper = int(amount * h * w * (1 - s_vs_p))

        for _ in range(num_salt):
            i = random.randint(0, h - 1)
            j = random.randint(0, w - 1)
            noisy[:, i, j] = 1.0

        for _ in range(num_pepper):
            i = random.randint(0, h - 1)
            j = random.randint(0, w - 1)
            noisy[:, i, j] = 0.0

        return noisy, {'type': 'salt_pepper', 'amount': amount, 's_vs_p': s_vs_p}

    def add_speckle_noise(self, img):
        scale = self.get_intensity_scale()
        noise_factor = random.uniform(0.05 * scale, 0.4 * scale)
        noise = noise_factor * torch.randn_like(img)
        noisy = torch.clamp(img + img * noise, 0., 1.)
        return noisy, {'type': 'speckle', 'factor': noise_factor}

    def apply_random_noise(self, img, allow_clean=True, clean_prob=0.1):
        if allow_clean and random.random() < clean_prob:
            return img.clone(), {'type': 'clean'}

        noise_type = random.choice(['gaussian', 'salt_pepper', 'speckle'])
        if noise_type == 'gaussian':
            return self.add_gaussian_noise(img)
        elif noise_type == 'salt_pepper':
            return self.add_salt_and_pepper_noise(img)
        else:
            return self.add_speckle_noise(img)

# Assume dataset or dataloader object has access to noise mixin
noise_mixin.set_epoch(current_epoch, total_epochs=100)

class MyCustomDenoisingDataset(torch.utils.data.Dataset, NoisyImageMixin):
    def __init__(self, ..., total_epochs=100):
        super().__init__()
        self.set_epoch(0, total_epochs)
        ...

    def set_training_epoch(self, epoch):
        self.set_epoch(epoch)

    def __getitem__(self, idx):
        clean_img = self.load_clean_image(idx)
        noisy_img, noise_info = self.apply_random_noise(clean_img)
        return noisy_img, clean_img, noise_info

# dataset = MyCustomDenoisingDataset(...)
for epoch in range(total_epochs):
    dataset.set_training_epoch(epoch)
    ...
    for noisy_img, clean_img, info in dataloader # i have no idea that there is no such a thing
        # train your model here

for epoch in range(1 , 5) :
  print(f"elena  is equal to {epoch}")







1 . uncertainty aware denoising the low - dose CT MICCAI 2020

2. noise2uncertainty CVPR 2021 paper

3 . Noise2Noise (Lehtinen et al., 2018)

4 . Denoising Diffusion Probabilistic Models (Ho et al., 2020)

5 . DAGAN (Kim et al., 2022)

6 . Self2Self (Quan et al., CVPR 2020)

7 . ESRGAN

8 . DnCNN-style models